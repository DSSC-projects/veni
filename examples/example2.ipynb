{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtyTP8_C79xR"
   },
   "source": [
    "# Training a MLP using forward and backward \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tutorial we look at the main features of **VENI**. In particular, we see how switch between forward automatic differentiation and backward automatic differentiation. For our aim we will train a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2PRtHa8IvXY"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import jax\n",
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "from veni import ReLU, Softmax, Sequential, Linear\n",
    "from veni.module import Module\n",
    "from veni.utils import NumpyLoader, one_hot\n",
    "from veni.optim import grad_fwd\n",
    "from veni.functiontools import CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6x8RCyW79xU"
   },
   "source": [
    "We now define a simple MLP and the network loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fuLi5I2ViLr9"
   },
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self):\n",
    "        self.layers = Sequential([\n",
    "            Linear(28*28, 1024, jax.random.PRNGKey(111)),\n",
    "            ReLU(),\n",
    "            Linear(1024, 10, jax.random.PRNGKey(222)),\n",
    "            Softmax()\n",
    "        ])\n",
    "\n",
    "        self.params = self.layers.generate_parameters()\n",
    "        # eliminate the bias\n",
    "\n",
    "    def forward(self, x, params):\n",
    "        return self.layers(x, params)\n",
    "\n",
    "\n",
    "model = MLP()\n",
    "params = model.params\n",
    "\n",
    "# loss + accuracy\n",
    "def loss(params, x, y):\n",
    "    y_hat = model(x, params)\n",
    "    return CrossEntropy(y, y_hat)\n",
    "\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    model_predictions = jnp.argmax(y_hat, axis=1)\n",
    "    return jnp.mean(y == model_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwMdEkYm79xV"
   },
   "source": [
    "Let's download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGG55g_OmppV",
    "outputId": "d4bf38ed-77b7-4a0f-e397-6cfb7d245de0"
   },
   "outputs": [],
   "source": [
    "class tf(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.array(np.ravel(pic), dtype=jnp.float32)/255\n",
    "\n",
    "# Define our dataset, using torch datasets\n",
    "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=tf(), train=True)\n",
    "training_generator = NumpyLoader(\n",
    "    mnist_dataset, batch_size=24, num_workers=0)\n",
    "\n",
    "mnist_dataset = MNIST('/tmp/mnist/', download=True,\n",
    "                      transform=tf(), train=False)\n",
    "testing_generator = NumpyLoader(\n",
    "    mnist_dataset, batch_size=24, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGztSd9S79xW"
   },
   "source": [
    "We define an update function, which can handle both backward and forward AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_bwd(params, x, y, loss, key):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update(params, x, y, loss, optimizer, key, grad_type = 'fwd'):\n",
    "    key = jax.random.split(key)\n",
    "    if grad_type == 'fwd':\n",
    "        grads = grad_fwd(params, x, y, loss, 1)\n",
    "    elif grad_type == 'bwd':\n",
    "        grads = grad_bwd(params, x, y, loss, key)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid grad_type, expected 'fwd' or 'bwd' got {grad_type}\")\n",
    "\n",
    "    return optimizer(params, grads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we are using vanilla SGD optimizer. Currently Adam and SGD optimizers are implemented in VENI: those optimizer are mainly used for simple benchmarking purpouses and are not meant to be efficient and fast.\n",
    "\n",
    "Now we can train the newtwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] / Accuracy [0.541700005531311]\n",
      "Epoch [1] / Accuracy [0.6349999904632568]\n",
      "Epoch [2] / Accuracy [0.6413000226020813]\n",
      "Epoch [3] / Accuracy [0.666100025177002]\n",
      "Epoch [4] / Accuracy [0.651199996471405]\n"
     ]
    }
   ],
   "source": [
    "#define the optimizer\n",
    "def optimizer(params, grad, eta = 2e-4):\n",
    "    return [(w - eta * dw, b - eta * db) for (w, b), (dw, db) in zip(params, grad)]\n",
    "\n",
    "key = jax.random.PRNGKey(111)\n",
    "for epoch in range(5):\n",
    "    for x, y in training_generator:\n",
    "        key = jax.random.split(key)\n",
    "        one_hot_label = one_hot(y, 10)\n",
    "        params = update(params, x, one_hot_label, loss, optimizer, key, grad_type='fwd')\n",
    "    \n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for x, y in testing_generator:\n",
    "        y_hat = model(x, params)\n",
    "        acc += accuracy(y, y_hat)*x.shape[0]\n",
    "        count += x.shape[0]\n",
    "    \n",
    "    print(f\"Epoch [{epoch}] / Accuracy [{acc / count}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now change freely from backward to forward or hybrid approaches."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "esempio_jax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "003cb6c4a39ad7570532a894863826d7c500b9b7e4857a0bcb5e0adb3dc45680"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
