{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training a MLP using backpropagation"
   ],
   "metadata": {
    "id": "QtyTP8_C79xR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following tutorial we look at the main features of **VINA**. In particular, we see how to define different classes of models, with an easy and user friendly interface."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **part 1** we train using `MLP`. \n",
    "* **part 2** we train using `Sequential` and `Linear`. \n",
    "* **part 3** we train using `Sequential`, `Linear` and `Conv2D`. "
   ],
   "metadata": {
    "id": "opkZuz6b79xT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad, jit\n",
    "from veni import ReLU, LogSoftmax, MLP, Module, Sequential, Linear, Conv2D, MaxPool2D, Flatten, Softmax\n",
    "from time import time\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu')"
   ],
   "outputs": [],
   "metadata": {
    "id": "t2PRtHa8IvXY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1\n",
    "We first define useful parameters that we are going to use later. Note that our model is a MLP imported from `net.py`"
   ],
   "metadata": {
    "id": "S6x8RCyW79xU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# hyperparameters\n",
    "layer_sizes = [784, 512, 256, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "key = jax.random.PRNGKey(0)\n",
    "function = ReLU()\n",
    "log_softmax = LogSoftmax()\n",
    "\n",
    "# to create a model is as simple as that\n",
    "model = MLP(layer_sizes, function, key)\n",
    "params = model.params # we need to extract parameters"
   ],
   "outputs": [],
   "metadata": {
    "id": "fuLi5I2ViLr9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper functions, trainloader and other stuff..."
   ],
   "metadata": {
    "id": "KwMdEkYm79xV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# some helper functions\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(model(images,params), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = model(images, params)\n",
    "    preds = log_softmax(preds)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "# Define our dataset, using torch datasets\n",
    "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)\n",
    "train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n",
    "\n",
    "# Get full test dataset\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\n",
    "test_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)\n",
    "test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/usr/local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGG55g_OmppV",
    "outputId": "d4bf38ed-77b7-4a0f-e397-6cfb7d245de0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we train using backpropagation"
   ],
   "metadata": {
    "id": "AGztSd9S79xW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time()\n",
    "    for x, y in training_generator:\n",
    "        y = one_hot(y, n_targets)\n",
    "        params = update(params, x, y)\n",
    "        epoch_time = time() - start_time\n",
    "\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 in 3.49 sec\n",
      "Training set accuracy 0.9045166373252869\n",
      "Test set accuracy 0.9120000004768372\n",
      "Epoch 1 in 3.04 sec\n",
      "Training set accuracy 0.9275500178337097\n",
      "Test set accuracy 0.9304999709129333\n",
      "Epoch 2 in 3.03 sec\n",
      "Training set accuracy 0.9414166808128357\n",
      "Test set accuracy 0.9398999810218811\n",
      "Epoch 3 in 3.18 sec\n",
      "Training set accuracy 0.9497833251953125\n",
      "Test set accuracy 0.9477999806404114\n",
      "Epoch 4 in 3.17 sec\n",
      "Training set accuracy 0.956416666507721\n",
      "Test set accuracy 0.9527999758720398\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWbp73vtq13K",
    "outputId": "9b5bd1c9-bf95-400c-e00e-31a96123f8ae"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "We first define useful parameters that we are going to use later. Note that our model is a costum model built using `Sequential` and `Linear`."
   ],
   "metadata": {
    "id": "dMyZN2La79xX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "class myModel(Module):\n",
    "    def __init__(self):\n",
    "        self.layers = Sequential([\n",
    "            Linear(28*28,512,key),\n",
    "            function,\n",
    "            Linear(512,256,key),\n",
    "            function,\n",
    "            Linear(256,10,key)\n",
    "        ])\n",
    "        self.params = self.layers.generate_parameters()\n",
    "\n",
    "    def forward(self, data, params):\n",
    "        return self.layers(data, params)\n",
    "\n",
    "\n",
    "# we define the model as simple as that\n",
    "model = myModel()\n",
    "params = model.params"
   ],
   "outputs": [],
   "metadata": {
    "id": "3T0BVKsJ79xY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train"
   ],
   "metadata": {
    "id": "HaaY-qwUi1In"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for x, y in training_generator:\n",
    "        y = one_hot(y, n_targets)\n",
    "        params = update(params, x, y)\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "  \n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 in 6.04 sec\n",
      "Training set accuracy 0.9094333648681641\n",
      "Test set accuracy 0.9138000011444092\n",
      "Epoch 1 in 5.02 sec\n",
      "Training set accuracy 0.9317333698272705\n",
      "Test set accuracy 0.9314999580383301\n",
      "Epoch 2 in 4.85 sec\n",
      "Training set accuracy 0.9435499906539917\n",
      "Test set accuracy 0.9408999681472778\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kpedeNf79xY",
    "outputId": "994c375f-dd99-4e66-dcd4-9e45b302f391"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3\n",
    "We first define useful parameters that we are going to use later. Note that our model is a costum model built using `Sequential`, `Linear` and `Conv2D` base on this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# hyperparameters\n",
    "step_size = 0.01\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "n_targets = 10\n",
    "key = jax.random.PRNGKey(0)\n",
    "function = ReLU()\n",
    "log_softmax = LogSoftmax()\n",
    "\n",
    "class Cast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.array(pic, dtype=jnp.float32)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), Cast()])\n",
    "\n",
    "cifar_dataset = CIFAR10('/tmp/cifar/', download=True,train=True,\n",
    "                        transform=transform)\n",
    "training_generator = NumpyLoader(\n",
    "    cifar_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# Get full test dataset\n",
    "cifar_dataset_test = CIFAR10('/tmp/cifar/', download=True, train=False,\n",
    "                             transform=Cast())\n",
    "testing_generator = NumpyLoader(\n",
    "    cifar_dataset, batch_size=batch_size, num_workers=0)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class CNNModel(Module):\n",
    "    def __init__(self):\n",
    "        self.layers = Sequential([\n",
    "            Conv2D(3, 6, 5, 1, 'VALID', jax.random.PRNGKey(0)),\n",
    "            function,\n",
    "            MaxPool2D(2, 2),\n",
    "            Conv2D(6, 16, 5, 1, 'VALID', jax.random.PRNGKey(1)),\n",
    "            function,\n",
    "            MaxPool2D(2, 2),\n",
    "            Flatten(),\n",
    "            Linear(16 * 5 * 5, 120, jax.random.PRNGKey(2)),\n",
    "            function,\n",
    "            Linear(120, 84, jax.random.PRNGKey(3)),\n",
    "            function,\n",
    "            Linear(84, 10, jax.random.PRNGKey(3))\n",
    "        ])\n",
    "        self.params = self.layers.generate_parameters()\n",
    "\n",
    "    def forward(self, data, params):\n",
    "        return self.layers(data, params)\n",
    "\n",
    "\n",
    "# we define the model as simple as that\n",
    "model = CNNModel()\n",
    "params = model.params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now train our model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, (image, label) in enumerate(training_generator):\n",
    "        one_hot_label = one_hot(label, n_targets)\n",
    "        params = update(params, image, one_hot_label)\n",
    "        \n",
    "        # loss info\n",
    "        loss_item =  loss(params, image, one_hot_label)\n",
    "        running_loss = running_loss + loss_item\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,  2000] loss: 0.216\n",
      "[1,  4000] loss: 0.186\n",
      "[1,  6000] loss: 0.165\n",
      "[1,  8000] loss: 0.153\n",
      "[1, 10000] loss: 0.152\n",
      "[1, 12000] loss: 0.145\n",
      "[2,  2000] loss: 0.137\n",
      "[2,  4000] loss: 0.138\n",
      "[2,  6000] loss: 0.132\n",
      "[2,  8000] loss: 0.129\n",
      "[2, 10000] loss: 0.132\n",
      "[2, 12000] loss: 0.128\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now validate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testing_generator:\n",
    "    images, labels = data\n",
    "    output = model(images,params)\n",
    "    predicted = jnp.argmax(Softmax()(output), 1)\n",
    "    total += labels.shape[0]\n",
    "    correct += float((predicted == labels).sum())\n",
    "    \n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 42.0 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "for data in testing_generator:\n",
    "    images, labels = data\n",
    "    output = model(images,params)\n",
    "    predicted = jnp.argmax(Softmax()(output), 1)\n",
    "    for label, prediction in zip(labels, predicted):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            \n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class: plane is 47.0 %\n",
      "Accuracy for class: car   is 77.6 %\n",
      "Accuracy for class: bird  is 21.1 %\n",
      "Accuracy for class: cat   is 24.7 %\n",
      "Accuracy for class: deer  is 38.2 %\n",
      "Accuracy for class: dog   is 16.6 %\n",
      "Accuracy for class: frog  is 50.3 %\n",
      "Accuracy for class: horse is 58.9 %\n",
      "Accuracy for class: ship  is 62.0 %\n",
      "Accuracy for class: truck is 29.5 %\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "esempio_jax.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
