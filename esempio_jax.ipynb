{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training a MLP using backpropagation and forward differentiation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **part 1** we train using backpropagation, while in **part 2** we train using forward ad."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from net import MLP\n",
        "from functional import relu, log_softmax\n",
        "from jax import grad, jit"
      ],
      "outputs": [],
      "metadata": {
        "id": "t2PRtHa8IvXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "We first define useful parameters that we are going to use later. Note that our model is a MLP imported from `net.py`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "# hyperparameters\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.01\n",
        "num_epochs = 8\n",
        "batch_size = 128\n",
        "n_targets = 10\n",
        "key = jax.random.PRNGKey(10)\n",
        "\n",
        "# to create a model is as simple as that\n",
        "model = MLP(layer_sizes, relu, key)\n",
        "params = model.params # we need to extract parameters"
      ],
      "outputs": [],
      "metadata": {
        "id": "m_Olitk7mB5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions, trainloader and other stuff..."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "# some helper functions\n",
        "def one_hot(x, k, dtype=jnp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
        "  \n",
        "def accuracy(params, images, targets):\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(model(params, images), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "def loss(params, images, targets):\n",
        "  preds = model(params, images)\n",
        "  preds = log_softmax(preds)\n",
        "  return -jnp.mean(preds * targets)\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return [(w - step_size * dw, b - step_size * db)\n",
        "          for (w, b), (dw, db) in zip(params, grads)]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def numpy_collate(batch):\n",
        "  if isinstance(batch[0], np.ndarray):\n",
        "    return np.stack(batch)\n",
        "  elif isinstance(batch[0], (tuple,list)):\n",
        "    transposed = zip(*batch)\n",
        "    return [numpy_collate(samples) for samples in transposed]\n",
        "  else:\n",
        "    return np.array(batch)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)\n",
        "\n",
        "class FlattenAndCast(object):\n",
        "  def __call__(self, pic):\n",
        "    return np.ravel(np.array(pic, dtype=jnp.float32))\n",
        "\n",
        "# Define our dataset, using torch datasets\n",
        "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
        "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "# Get the full train dataset (for checking accuracy while training)\n",
        "train_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)\n",
        "train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n",
        "\n",
        "# Get full test dataset\n",
        "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\n",
        "test_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)\n",
        "test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGG55g_OmppV",
        "outputId": "e0f7b13b-778c-46d1-dd32-5dd76fcc5b7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we train using backpropagation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "import time\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  for x, y in training_generator:\n",
        "    y = one_hot(y, n_targets)\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  train_acc = accuracy(params, train_images, train_labels)\n",
        "  test_acc = accuracy(params, test_images, test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 in 9.15 sec\n",
            "Training set accuracy 0.9136999845504761\n",
            "Test set accuracy 0.9180999994277954\n",
            "Epoch 1 in 8.54 sec\n",
            "Training set accuracy 0.9374666810035706\n",
            "Test set accuracy 0.9391999840736389\n",
            "Epoch 2 in 8.06 sec\n",
            "Training set accuracy 0.9483166933059692\n",
            "Test set accuracy 0.9483999609947205\n",
            "Epoch 3 in 8.11 sec\n",
            "Training set accuracy 0.9550333619117737\n",
            "Test set accuracy 0.9530999660491943\n",
            "Epoch 4 in 8.00 sec\n",
            "Training set accuracy 0.9599666595458984\n",
            "Test set accuracy 0.9565999507904053\n",
            "Epoch 5 in 8.00 sec\n",
            "Training set accuracy 0.9637500047683716\n",
            "Test set accuracy 0.9598000049591064\n",
            "Epoch 6 in 7.99 sec\n",
            "Training set accuracy 0.9667500257492065\n",
            "Test set accuracy 0.9623000025749207\n",
            "Epoch 7 in 8.25 sec\n",
            "Training set accuracy 0.9694833159446716\n",
            "Test set accuracy 0.9633999466896057\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWbp73vtq13K",
        "outputId": "437d0558-99fd-4c80-e085-25335797ade1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "\n",
        "Now lets's train using forward AD. Note that we need to change a bit the function `update`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "from jax import jvp\n",
        "\n",
        "def get_vector(key, params):\n",
        "  v_shaped = []\n",
        "  for w,b in params:\n",
        "    key, subkey = jax.random.split(key)\n",
        "    v_w = jax.random.normal(key, shape=w.shape)\n",
        "    v_b = jax.random.normal(subkey, shape=b.shape)\n",
        "    v_shaped.append((v_w, v_b))\n",
        "  return v_shaped\n",
        "\n",
        "@jit\n",
        "def update(params, key, x, y):\n",
        "  v = get_vector(key, params)\n",
        "  _, dd = jvp(lambda params: loss(params, x, y), (params,), (v,))\n",
        "  step = step_size * dd\n",
        "  return [(w - step * dw, b - step * db)\n",
        "          for (w, b), (dw, db) in zip(params, v)]"
      ],
      "outputs": [],
      "metadata": {
        "id": "ohBbGUPrtNzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train again"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "step_size = 2e-5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  for x, y in training_generator:\n",
        "    key, _ = jax.random.split(key)\n",
        "    y = one_hot(y, n_targets)\n",
        "    params = update(params, key, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  train_acc = accuracy(params, train_images, train_labels)\n",
        "  test_acc = accuracy(params, test_images, test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 in 13.03 sec\n",
            "Training set accuracy 0.3375166654586792\n",
            "Test set accuracy 0.3328000009059906\n",
            "Epoch 1 in 12.73 sec\n",
            "Training set accuracy 0.5393666625022888\n",
            "Test set accuracy 0.5331000089645386\n",
            "Epoch 2 in 12.53 sec\n",
            "Training set accuracy 0.6308666467666626\n",
            "Test set accuracy 0.6401000022888184\n",
            "Epoch 3 in 15.68 sec\n",
            "Training set accuracy 0.6854166984558105\n",
            "Test set accuracy 0.6976999640464783\n",
            "Epoch 4 in 15.73 sec\n",
            "Training set accuracy 0.7157333493232727\n",
            "Test set accuracy 0.7224999666213989\n",
            "Epoch 5 in 12.70 sec\n",
            "Training set accuracy 0.7372666597366333\n",
            "Test set accuracy 0.7444999814033508\n",
            "Epoch 6 in 13.06 sec\n",
            "Training set accuracy 0.7376000285148621\n",
            "Test set accuracy 0.7483999729156494\n",
            "Epoch 7 in 12.83 sec\n",
            "Training set accuracy 0.7309666872024536\n",
            "Test set accuracy 0.7409999966621399\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exwPeezQusrf",
        "outputId": "fd7177ec-a7ec-4c96-cdf4-98ecd41d5171"
      }
    }
  ]
}