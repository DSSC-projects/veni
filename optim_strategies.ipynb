{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calcolo del gradiente con più direzioni**\n",
    "\n",
    " - In questo caso dobbiamo, per ogni time-step, calcolare il gradiente scegliendo più direzioni. La cosa migliore sarebbe quella di fare una funzione (guarda Appendix 1)\n",
    "\n",
    " - Una volta visto che funziona, proverei a riprodurre un grafico tipo quello di DOODGE per logistic regression mnist con SGD. In particolare plottiamo accuracy in ordinata e epoch in ascissa e facciamo più curve: 1 curva rappresenta SGD con gradiente calcolato con 1 direzione, poi 4 curve rappresentanti SGD con gradiente calcolato con 5, 10, 20, 40 direzioni, ed una curva rappresentante SGD con gradiente calcolato con backpropagation. Per questi test learning rate fisso a quello che abbiamo, i.e. 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading into/from path ../datasets\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from veni.net import *\n",
    "from veni.functiontools import *\n",
    "from veni.function import *\n",
    "from veni.utils import * \n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "jax.config.update('jax_platform_name','cpu')\n",
    "\n",
    "dspath = '../datasets'\n",
    "batch_size = 24\n",
    "num_epochs = 5 \n",
    "n_targets = 10\n",
    "step_size = 1e-4\n",
    "\n",
    "s0 = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(111111)\n",
    "\n",
    "logging_freq = 200\n",
    "\n",
    "class tf(object):\n",
    "    def __call__(self, pic):\n",
    "        return ( np.array(np.ravel(pic), dtype=jnp.float32)/ 255. - 0.5 ) * 2\n",
    "\n",
    "print(f\"Loading into/from path {dspath}\")\n",
    "train_dataset = MNIST(dspath, train = True, download= True, transform= tf())\n",
    "\n",
    "n_train = train_dataset.data.shape[0]\n",
    "train_generator = NumpyLoader(train_dataset, batch_size= batch_size )\n",
    "\n",
    "\n",
    "test_dataset = MNIST(dspath, train = False,download= True, transform= tf() )\n",
    "\n",
    "n_test = train_dataset.data.shape[0]\n",
    "test_generator = NumpyLoader(test_dataset, batch_size= batch_size )\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(Module):\n",
    "    def __init__(self):\n",
    "        self.layers = Sequential([\n",
    "            Linear(28*28, 10, key),\n",
    "            Sigmoid()\n",
    "        ])\n",
    "\n",
    "        self.params = self.layers.generate_parameters()\n",
    "    \n",
    "    def forward(self,x,params):\n",
    "        return self.layers(x,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(Module):\n",
    "    def __init__(self):\n",
    "        self.layers = Sequential([\n",
    "            Linear(28*28, 1024, key),\n",
    "            ReLU(),\n",
    "            Linear(1024, 10, key),\n",
    "            Softmax()\n",
    "        ])\n",
    "\n",
    "        self.params = self.layers.generate_parameters()\n",
    "    \n",
    "    def forward(self,x,params):\n",
    "        return self.layers(x,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressor()\n",
    "\n",
    "def logisticLoss(y,y_hat):\n",
    "    l1 = y*jnp.log(y_hat)\n",
    "    l2 = (1-y)*jnp.log(1 - y_hat)\n",
    "    l = -jnp.sum(l1 + l2)\n",
    "    return l/y.shape[0]\n",
    "\n",
    "def sample_random_direction(params, normalize = False):\n",
    "    global key\n",
    "    v_shaped = []\n",
    "    for w, b in params:\n",
    "        key, subkey = random.split(key)\n",
    "        v_w = random.normal(key, shape=w.shape)\n",
    "        v_b = random.normal(subkey, shape=b.shape)\n",
    "        if normalize:\n",
    "            v_shaped.append((v_w/jnp.linalg.norm(v_w), v_b/jnp.linalg.norm(v_b)))\n",
    "        else:\n",
    "            v_shaped.append((v_w,v_b))\n",
    "    return v_shaped\n",
    "\n",
    "\n",
    "def update_params_list(p,g,scale = 1.):\n",
    "    return [ (w - dw*scale, b - db*scale) for (w,b), (dw,db) in zip(p,g)]\n",
    "\n",
    "def accuracy(y,y_hat):\n",
    "    n = y.shape[0]\n",
    "    y = jnp.argmax(y, axis = 1)\n",
    "    y_hat = jnp.argmax(y_hat, axis = 1)\n",
    "\n",
    "    return jnp.sum(y == y_hat)/n\n",
    "\n",
    "def train_loop(model, train_generator, validation_generator, loss, nepochs, batchIteration, lr, log_every_n_batches):  \n",
    "    params = model.params\n",
    "    global key\n",
    "    train_loss = [] \n",
    "    validation_loss = [] \n",
    "\n",
    "    train_acc = []\n",
    "    validation_acc = []\n",
    "\n",
    "    def lossFn(params, x, y):\n",
    "        y_hat = model(x,params)\n",
    "        return loss(y,y_hat)\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        print(f\"--- Epoch : {epoch} ---\")\n",
    "        l = 0\n",
    "        a = 0\n",
    "        print(f\"\\t Training\")\n",
    "        for i, (x,y) in enumerate(train_generator):\n",
    "            key = jax.random.split(key)\n",
    "            one_hot_y = one_hot(y,n_targets)\n",
    "            params = batchIteration(params,x,one_hot_y,lossFn, lr)\n",
    "            y_hat = model(x, params)\n",
    "            l += loss(one_hot_y,y_hat)\n",
    "            a += accuracy(one_hot_y,y_hat)\n",
    "\n",
    "            if i % log_every_n_batches == (log_every_n_batches - 1):\n",
    "                train_loss.append([epoch,i,float(l)/log_every_n_batches])\n",
    "                train_acc.append([epoch,i,float(a)/log_every_n_batches])\n",
    "                l = 0\n",
    "                a = 0\n",
    "\n",
    "                print(f\"\\t l: {train_loss[-1]}\")\n",
    "                print(f\"\\t a: {train_acc[-1]}\")\n",
    "\n",
    "        l = 0\n",
    "        a = 0\n",
    "        \n",
    "        for i, (x,y) in enumerate(validation_generator):\n",
    "            one_hot_y = one_hot(y,n_targets)\n",
    "            y_hat = model(x,params)\n",
    "            l += loss(one_hot_y,y_hat)\n",
    "            a += accuracy(one_hot_y,y_hat)\n",
    "\n",
    "        validation_loss.append([epoch,i,float(l)/ i])\n",
    "        validation_acc.append([epoch,i,float(a)/ i])\n",
    "        print(f\"\\t Validation\")\n",
    "        print(f\"\\t l: {validation_loss[-1]}\")\n",
    "        print(f\"\\t a: {validation_acc[-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "    return [train_loss, train_acc, validation_loss, validation_acc]\n",
    "\n",
    "def bwd_train_batch(params, x, y, lossFn, lr):\n",
    "    grad = jax.grad(lossFn)(params,x,y)\n",
    "    params = update_params_list(params,grad, lr)\n",
    "    return params\n",
    "\n",
    "def single_fwd_train_batch(params,x,y,lossFn,lr):\n",
    "    global key\n",
    "    key = jax.random.split(key)\n",
    "    v = sample_random_direction(params, normalize=False)\n",
    "    _, j = jax.jvp(lambda p: lossFn(p,x,y), (params, ), (v,))\n",
    "    params = update_params_list(params, v, j*lr)\n",
    "    return params\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 5.516746215820312]\n",
      "\t a: [0, 199, 0.105390625]\n",
      "\t l: [0, 399, 3.6201287841796876]\n",
      "\t a: [0, 399, 0.145703125]\n",
      "\t l: [0, 599, 3.42521240234375]\n",
      "\t a: [0, 599, 0.158671875]\n",
      "\t l: [0, 799, 3.3166775512695312]\n",
      "\t a: [0, 799, 0.183359375]\n",
      "\t Validation\n",
      "\t l: [0, 156, 2.5286004638671873]\n",
      "\t a: [0, 156, 0.171796875]\n"
     ]
    }
   ],
   "source": [
    "m = LogisticRegressor()\n",
    "values = train_loop(m,train_generator, test_generator, logisticLoss, 1, single_fwd_train_batch, 1e-4, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def graph_all(values, btrain, btest, title = 'Title'):\n",
    "    train_loss = values[0]\n",
    "    train_loss = np.array([[l[0]*btrain + l[1], l[2]] for l in train_loss])\n",
    "\n",
    "    train_acc = values[1]\n",
    "    train_acc = np.array([[l[0]*btrain + l[1], l[2]] for l in train_acc])\n",
    "    \n",
    "    validation_loss = values[2]\n",
    "    validation_loss = np.array([[l[0]*btest + l[1], l[2]] for l in validation_loss])\n",
    "\n",
    "    validation_acc = values[3]\n",
    "    validation_acc = np.array([[l[0]*btest + l[1], l[2]] for l in validation_acc])\n",
    "\n",
    "    plt.figure(figsize= (15,10))\n",
    "    plt.title(title)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title(\"train loss\")\n",
    "    plt.plot(train_loss[:,0], train_loss[:,1], 'b.')\n",
    "\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title(\"train acc\")\n",
    "    plt.plot(train_acc[:,0], train_acc[:,1], 'r.')\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title(\"validation loss\")\n",
    "    plt.plot(validation_loss[:,0], validation_loss[:,1], 'y.')\n",
    "\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title(\"validation acc\")\n",
    "    plt.plot(validation_acc[:,0], validation_acc[:,1], 'k.')\n",
    "graph_all(values, btrain = n_train // batch_size, btest = n_test // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_fwd_train_batch(params,x,y,lossFn,lr, dirs ):\n",
    "    global key\n",
    "    for i in range(dirs):\n",
    "        key, _ = jax.random.split(key)\n",
    "        v = sample_random_direction(params, normalize= False)\n",
    "        _, j = jax.jvp(lambda p: lossFn(p, x, y), (params, ), (v,))\n",
    "        grad_now = [(j * dw, j * db) for (dw, db) in v]\n",
    "\n",
    "        if i == 0:\n",
    "            grad = grad_now\n",
    "\n",
    "        elif i > 0:\n",
    "            grad = [(dw + dw_prev, db + db_prev)\n",
    "                           for (dw, db), (dw_prev, db_prev) in zip(grad_now, grad)]\n",
    "\n",
    "\n",
    "    if dirs != 1:\n",
    "        grad = [(dw / dirs, db / dirs) for (dw, db) in grad]\n",
    "\n",
    "    params = update_params_list(params,grad,lr) \n",
    "    return params \n",
    "\n",
    "\n",
    "#passare ognuna di queste al training loop per fare il confronto con multiple direzioni\n",
    "\n",
    "m5_fwd_train_batch = lambda params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, 5)\n",
    "m10_fwd_train_batch = lambda params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, 10)\n",
    "m20_fwd_train_batch = lambda params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, 20)\n",
    "m40_fwd_train_batch = lambda params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 5.0870599365234375]\n",
      "\t a: [0, 199, 0.096484375]\n",
      "\t l: [0, 399, 3.512740478515625]\n",
      "\t a: [0, 399, 0.130859375]\n",
      "\t l: [0, 599, 3.3507232666015625]\n",
      "\t a: [0, 599, 0.161015625]\n",
      "\t l: [0, 799, 3.248712463378906]\n",
      "\t a: [0, 799, 0.1990625]\n",
      "\t Validation\n",
      "\t l: [0, 156, 2.4795372009277346]\n",
      "\t a: [0, 156, 0.181328125]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 3.122544250488281]\n",
      "\t a: [1, 199, 0.240703125]\n",
      "\t l: [1, 399, 3.0541763305664062]\n",
      "\t a: [1, 399, 0.277421875]\n",
      "\t l: [1, 599, 2.9861376953125]\n",
      "\t a: [1, 599, 0.293671875]\n",
      "\t l: [1, 799, 2.9070098876953123]\n",
      "\t a: [1, 799, 0.328515625]\n",
      "\t Validation\n",
      "\t l: [1, 156, 2.2140623474121095]\n",
      "\t a: [1, 156, 0.28921875]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.7971810913085937]\n",
      "\t a: [2, 199, 0.3790625]\n",
      "\t l: [2, 399, 2.757724609375]\n",
      "\t a: [2, 399, 0.40375]\n",
      "\t l: [2, 599, 2.714082946777344]\n",
      "\t a: [2, 599, 0.41859375]\n",
      "\t l: [2, 799, 2.6520755004882814]\n",
      "\t a: [2, 799, 0.44390625]\n",
      "\t Validation\n",
      "\t l: [2, 156, 2.0173956298828126]\n",
      "\t a: [2, 156, 0.38859375]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 2.556652069091797]\n",
      "\t a: [3, 199, 0.50015625]\n",
      "\t l: [3, 399, 2.5277281188964844]\n",
      "\t a: [3, 399, 0.505390625]\n",
      "\t l: [3, 599, 2.4952078247070313]\n",
      "\t a: [3, 599, 0.5175]\n",
      "\t l: [3, 799, 2.4434033203125]\n",
      "\t a: [3, 799, 0.54296875]\n",
      "\t Validation\n",
      "\t l: [3, 156, 1.8630572509765626]\n",
      "\t a: [3, 156, 0.452109375]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 2.3669618225097655]\n",
      "\t a: [4, 199, 0.577265625]\n",
      "\t l: [4, 399, 2.3470140075683594]\n",
      "\t a: [4, 399, 0.5796875]\n",
      "\t l: [4, 599, 2.3222608947753907]\n",
      "\t a: [4, 599, 0.58828125]\n",
      "\t l: [4, 799, 2.2816844177246094]\n",
      "\t a: [4, 799, 0.602265625]\n",
      "\t Validation\n",
      "\t l: [4, 156, 1.7403396606445312]\n",
      "\t a: [4, 156, 0.494765625]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 2.2131167602539064]\n",
      "\t a: [5, 199, 0.62953125]\n",
      "\t l: [5, 399, 2.2026235961914065]\n",
      "\t a: [5, 399, 0.62625]\n",
      "\t l: [5, 599, 2.185496826171875]\n",
      "\t a: [5, 599, 0.63078125]\n",
      "\t l: [5, 799, 2.150695343017578]\n",
      "\t a: [5, 799, 0.645]\n",
      "\t Validation\n",
      "\t l: [5, 156, 1.6378713989257812]\n",
      "\t a: [5, 156, 0.5271875]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 2.082988586425781]\n",
      "\t a: [6, 199, 0.66875]\n",
      "\t l: [6, 399, 2.0792274475097656]\n",
      "\t a: [6, 399, 0.66296875]\n",
      "\t l: [6, 599, 2.066935577392578]\n",
      "\t a: [6, 599, 0.6665625]\n",
      "\t l: [6, 799, 2.0365411376953126]\n",
      "\t a: [6, 799, 0.675]\n",
      "\t Validation\n",
      "\t l: [6, 156, 1.5511328125]\n",
      "\t a: [6, 156, 0.55203125]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.9775965881347657]\n",
      "\t a: [7, 199, 0.6975]\n",
      "\t l: [7, 399, 1.9837132263183594]\n",
      "\t a: [7, 399, 0.686171875]\n",
      "\t l: [7, 599, 1.9772998046875]\n",
      "\t a: [7, 599, 0.6946875]\n",
      "\t l: [7, 799, 1.9493598937988281]\n",
      "\t a: [7, 799, 0.703046875]\n",
      "\t Validation\n",
      "\t l: [7, 156, 1.4851412963867188]\n",
      "\t a: [7, 156, 0.569453125]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.8979869079589844]\n",
      "\t a: [8, 199, 0.713671875]\n",
      "\t l: [8, 399, 1.9053474426269532]\n",
      "\t a: [8, 399, 0.70546875]\n",
      "\t l: [8, 599, 1.900844268798828]\n",
      "\t a: [8, 599, 0.71515625]\n",
      "\t l: [8, 799, 1.8760934448242188]\n",
      "\t a: [8, 799, 0.720390625]\n",
      "\t Validation\n",
      "\t l: [8, 156, 1.4271176147460938]\n",
      "\t a: [8, 156, 0.5828125]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.8229280090332032]\n",
      "\t a: [9, 199, 0.73125]\n",
      "\t l: [9, 399, 1.8312631225585938]\n",
      "\t a: [9, 399, 0.719765625]\n",
      "\t l: [9, 599, 1.8273526000976563]\n",
      "\t a: [9, 599, 0.730078125]\n",
      "\t l: [9, 799, 1.805354766845703]\n",
      "\t a: [9, 799, 0.73734375]\n",
      "\t Validation\n",
      "\t l: [9, 156, 1.3730685424804687]\n",
      "\t a: [9, 156, 0.59546875]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 6.2726593017578125]\n",
      "\t a: [0, 199, 0.157890625]\n",
      "\t l: [0, 399, 3.421941223144531]\n",
      "\t a: [0, 399, 0.176640625]\n",
      "\t l: [0, 599, 3.224329833984375]\n",
      "\t a: [0, 599, 0.194140625]\n",
      "\t l: [0, 799, 3.127943420410156]\n",
      "\t a: [0, 799, 0.224609375]\n",
      "\t Validation\n",
      "\t l: [0, 156, 2.3878692626953124]\n",
      "\t a: [0, 156, 0.201328125]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.9974267578125]\n",
      "\t a: [1, 199, 0.276796875]\n",
      "\t l: [1, 399, 2.957300720214844]\n",
      "\t a: [1, 399, 0.29078125]\n",
      "\t l: [1, 599, 2.892481384277344]\n",
      "\t a: [1, 599, 0.317421875]\n",
      "\t l: [1, 799, 2.822734375]\n",
      "\t a: [1, 799, 0.350546875]\n",
      "\t Validation\n",
      "\t l: [1, 156, 2.1519834899902346]\n",
      "\t a: [1, 156, 0.308515625]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.70728515625]\n",
      "\t a: [2, 199, 0.406875]\n",
      "\t l: [2, 399, 2.68022216796875]\n",
      "\t a: [2, 399, 0.416328125]\n",
      "\t l: [2, 599, 2.63087890625]\n",
      "\t a: [2, 599, 0.441328125]\n",
      "\t l: [2, 799, 2.577616882324219]\n",
      "\t a: [2, 799, 0.470390625]\n",
      "\t Validation\n",
      "\t l: [2, 156, 1.9651390075683595]\n",
      "\t a: [2, 156, 0.400390625]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 2.4766390991210936]\n",
      "\t a: [3, 199, 0.518359375]\n",
      "\t l: [3, 399, 2.4646182250976563]\n",
      "\t a: [3, 399, 0.520546875]\n",
      "\t l: [3, 599, 2.425653381347656]\n",
      "\t a: [3, 599, 0.539765625]\n",
      "\t l: [3, 799, 2.379566955566406]\n",
      "\t a: [3, 799, 0.558359375]\n",
      "\t Validation\n",
      "\t l: [3, 156, 1.8152037048339844]\n",
      "\t a: [3, 156, 0.466640625]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 2.292111053466797]\n",
      "\t a: [4, 199, 0.601484375]\n",
      "\t l: [4, 399, 2.293594970703125]\n",
      "\t a: [4, 399, 0.596953125]\n",
      "\t l: [4, 599, 2.2697003173828123]\n",
      "\t a: [4, 599, 0.60609375]\n",
      "\t l: [4, 799, 2.2304415893554688]\n",
      "\t a: [4, 799, 0.6228125]\n",
      "\t Validation\n",
      "\t l: [4, 156, 1.6998611450195313]\n",
      "\t a: [4, 156, 0.50765625]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 2.1495242309570313]\n",
      "\t a: [5, 199, 0.6553125]\n",
      "\t l: [5, 399, 2.1578065490722658]\n",
      "\t a: [5, 399, 0.643828125]\n",
      "\t l: [5, 599, 2.139349365234375]\n",
      "\t a: [5, 599, 0.651328125]\n",
      "\t l: [5, 799, 2.102793426513672]\n",
      "\t a: [5, 799, 0.666796875]\n",
      "\t Validation\n",
      "\t l: [5, 156, 1.605381317138672]\n",
      "\t a: [5, 156, 0.53796875]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 2.032076110839844]\n",
      "\t a: [6, 199, 0.6928125]\n",
      "\t l: [6, 399, 2.047178192138672]\n",
      "\t a: [6, 399, 0.68265625]\n",
      "\t l: [6, 599, 2.030919189453125]\n",
      "\t a: [6, 599, 0.68640625]\n",
      "\t l: [6, 799, 2.000083770751953]\n",
      "\t a: [6, 799, 0.69828125]\n",
      "\t Validation\n",
      "\t l: [6, 156, 1.5279873657226561]\n",
      "\t a: [6, 156, 0.561171875]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.935093536376953]\n",
      "\t a: [7, 199, 0.7178125]\n",
      "\t l: [7, 399, 1.953914794921875]\n",
      "\t a: [7, 399, 0.706328125]\n",
      "\t l: [7, 599, 1.9432022094726562]\n",
      "\t a: [7, 599, 0.709609375]\n",
      "\t l: [7, 799, 1.9139677429199218]\n",
      "\t a: [7, 799, 0.718828125]\n",
      "\t Validation\n",
      "\t l: [7, 156, 1.4612132263183595]\n",
      "\t a: [7, 156, 0.576640625]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.8520820617675782]\n",
      "\t a: [8, 199, 0.73609375]\n",
      "\t l: [8, 399, 1.8750701904296876]\n",
      "\t a: [8, 399, 0.725]\n",
      "\t l: [8, 599, 1.8669134521484374]\n",
      "\t a: [8, 599, 0.72578125]\n",
      "\t l: [8, 799, 1.8420199584960937]\n",
      "\t a: [8, 799, 0.735234375]\n",
      "\t Validation\n",
      "\t l: [8, 156, 1.4055683898925782]\n",
      "\t a: [8, 156, 0.590859375]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.7828947448730468]\n",
      "\t a: [9, 199, 0.750859375]\n",
      "\t l: [9, 399, 1.8064199829101562]\n",
      "\t a: [9, 399, 0.741171875]\n",
      "\t l: [9, 599, 1.7992851257324218]\n",
      "\t a: [9, 599, 0.740625]\n",
      "\t l: [9, 799, 1.7756080627441406]\n",
      "\t a: [9, 799, 0.74890625]\n",
      "\t Validation\n",
      "\t l: [9, 156, 1.355506134033203]\n",
      "\t a: [9, 156, 0.59890625]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 4.7707431030273435]\n",
      "\t a: [0, 199, 0.108203125]\n",
      "\t l: [0, 399, 3.2691909790039064]\n",
      "\t a: [0, 399, 0.1634375]\n",
      "\t l: [0, 599, 3.1387283325195314]\n",
      "\t a: [0, 599, 0.20625]\n",
      "\t l: [0, 799, 3.0528057861328124]\n",
      "\t a: [0, 799, 0.2528125]\n",
      "\t Validation\n",
      "\t l: [0, 156, 2.328971099853516]\n",
      "\t a: [0, 156, 0.230703125]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.913382873535156]\n",
      "\t a: [1, 199, 0.334140625]\n",
      "\t l: [1, 399, 2.868262939453125]\n",
      "\t a: [1, 399, 0.355703125]\n",
      "\t l: [1, 599, 2.8198095703125]\n",
      "\t a: [1, 599, 0.380625]\n",
      "\t l: [1, 799, 2.7563650512695315]\n",
      "\t a: [1, 799, 0.416640625]\n",
      "\t Validation\n",
      "\t l: [1, 156, 2.0996180725097657]\n",
      "\t a: [1, 156, 0.352421875]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.63190185546875]\n",
      "\t a: [2, 199, 0.4740625]\n",
      "\t l: [2, 399, 2.6022625732421876]\n",
      "\t a: [2, 399, 0.477265625]\n",
      "\t l: [2, 599, 2.5670147705078126]\n",
      "\t a: [2, 599, 0.49359375]\n",
      "\t l: [2, 799, 2.5144808959960936]\n",
      "\t a: [2, 799, 0.520859375]\n",
      "\t Validation\n",
      "\t l: [2, 156, 1.9147059631347656]\n",
      "\t a: [2, 156, 0.430703125]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 2.4091358947753907]\n",
      "\t a: [3, 199, 0.56671875]\n",
      "\t l: [3, 399, 2.3964044189453126]\n",
      "\t a: [3, 399, 0.562890625]\n",
      "\t l: [3, 599, 2.3708650207519533]\n",
      "\t a: [3, 599, 0.56875]\n",
      "\t l: [3, 799, 2.3281478881835938]\n",
      "\t a: [3, 799, 0.59140625]\n",
      "\t Validation\n",
      "\t l: [3, 156, 1.7738551330566406]\n",
      "\t a: [3, 156, 0.482265625]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 2.2381936645507814]\n",
      "\t a: [4, 199, 0.62671875]\n",
      "\t l: [4, 399, 2.2361460876464845]\n",
      "\t a: [4, 399, 0.61921875]\n",
      "\t l: [4, 599, 2.221871795654297]\n",
      "\t a: [4, 599, 0.6234375]\n",
      "\t l: [4, 799, 2.1863742065429688]\n",
      "\t a: [4, 799, 0.635859375]\n",
      "\t Validation\n",
      "\t l: [4, 156, 1.6649537658691407]\n",
      "\t a: [4, 156, 0.51609375]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 2.104436798095703]\n",
      "\t a: [5, 199, 0.666171875]\n",
      "\t l: [5, 399, 2.1084445190429686]\n",
      "\t a: [5, 399, 0.659765625]\n",
      "\t l: [5, 599, 2.099085235595703]\n",
      "\t a: [5, 599, 0.66390625]\n",
      "\t l: [5, 799, 2.068667144775391]\n",
      "\t a: [5, 799, 0.669375]\n",
      "\t Validation\n",
      "\t l: [5, 156, 1.5746437072753907]\n",
      "\t a: [5, 156, 0.541953125]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 1.9939262390136718]\n",
      "\t a: [6, 199, 0.698203125]\n",
      "\t l: [6, 399, 2.0032574462890627]\n",
      "\t a: [6, 399, 0.6859375]\n",
      "\t l: [6, 599, 1.9951812744140625]\n",
      "\t a: [6, 599, 0.690234375]\n",
      "\t l: [6, 799, 1.9683164978027343]\n",
      "\t a: [6, 799, 0.694296875]\n",
      "\t Validation\n",
      "\t l: [6, 156, 1.4983218383789063]\n",
      "\t a: [6, 156, 0.564609375]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.8988270568847656]\n",
      "\t a: [7, 199, 0.723671875]\n",
      "\t l: [7, 399, 1.9143197631835938]\n",
      "\t a: [7, 399, 0.70953125]\n",
      "\t l: [7, 599, 1.91100341796875]\n",
      "\t a: [7, 599, 0.71234375]\n",
      "\t l: [7, 799, 1.8876431274414063]\n",
      "\t a: [7, 799, 0.71515625]\n",
      "\t Validation\n",
      "\t l: [7, 156, 1.43672607421875]\n",
      "\t a: [7, 156, 0.577890625]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.8239892578125]\n",
      "\t a: [8, 199, 0.739140625]\n",
      "\t l: [8, 399, 1.84199462890625]\n",
      "\t a: [8, 399, 0.725703125]\n",
      "\t l: [8, 599, 1.8404391479492188]\n",
      "\t a: [8, 599, 0.729140625]\n",
      "\t l: [8, 799, 1.8187428283691407]\n",
      "\t a: [8, 799, 0.729765625]\n",
      "\t Validation\n",
      "\t l: [8, 156, 1.3823764038085937]\n",
      "\t a: [8, 156, 0.591484375]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.7567671203613282]\n",
      "\t a: [9, 199, 0.754609375]\n",
      "\t l: [9, 399, 1.7769903564453124]\n",
      "\t a: [9, 399, 0.74234375]\n",
      "\t l: [9, 599, 1.7769796752929687]\n",
      "\t a: [9, 599, 0.74546875]\n",
      "\t l: [9, 799, 1.7572352600097656]\n",
      "\t a: [9, 799, 0.74640625]\n",
      "\t Validation\n",
      "\t l: [9, 156, 1.335321807861328]\n",
      "\t a: [9, 156, 0.59875]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 5.46926025390625]\n",
      "\t a: [0, 199, 0.116484375]\n",
      "\t l: [0, 399, 3.4288339233398437]\n",
      "\t a: [0, 399, 0.142890625]\n",
      "\t l: [0, 599, 3.2768295288085936]\n",
      "\t a: [0, 599, 0.16078125]\n",
      "\t l: [0, 799, 3.1943017578125]\n",
      "\t a: [0, 799, 0.194140625]\n",
      "\t Validation\n",
      "\t l: [0, 156, 2.422420654296875]\n",
      "\t a: [0, 156, 0.190234375]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 3.0348800659179687]\n",
      "\t a: [1, 199, 0.275703125]\n",
      "\t l: [1, 399, 2.979317321777344]\n",
      "\t a: [1, 399, 0.30109375]\n",
      "\t l: [1, 599, 2.927845764160156]\n",
      "\t a: [1, 599, 0.329765625]\n",
      "\t l: [1, 799, 2.8655581665039063]\n",
      "\t a: [1, 799, 0.357109375]\n",
      "\t Validation\n",
      "\t l: [1, 156, 2.172139129638672]\n",
      "\t a: [1, 156, 0.31890625]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.725305480957031]\n",
      "\t a: [2, 199, 0.426875]\n",
      "\t l: [2, 399, 2.6927783203125]\n",
      "\t a: [2, 399, 0.4415625]\n",
      "\t l: [2, 599, 2.6585809326171876]\n",
      "\t a: [2, 599, 0.45171875]\n",
      "\t l: [2, 799, 2.608673095703125]\n",
      "\t a: [2, 799, 0.482421875]\n",
      "\t Validation\n",
      "\t l: [2, 156, 1.981826934814453]\n",
      "\t a: [2, 156, 0.406171875]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 2.4924339294433593]\n",
      "\t a: [3, 199, 0.533984375]\n",
      "\t l: [3, 399, 2.4753257751464846]\n",
      "\t a: [3, 399, 0.537734375]\n",
      "\t l: [3, 599, 2.4513534545898437]\n",
      "\t a: [3, 599, 0.543203125]\n",
      "\t l: [3, 799, 2.4080091857910157]\n",
      "\t a: [3, 799, 0.56734375]\n",
      "\t Validation\n",
      "\t l: [3, 156, 1.828791046142578]\n",
      "\t a: [3, 156, 0.472734375]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 2.3041366577148437]\n",
      "\t a: [4, 199, 0.610078125]\n",
      "\t l: [4, 399, 2.2980975341796874]\n",
      "\t a: [4, 399, 0.608359375]\n",
      "\t l: [4, 599, 2.2844390869140625]\n",
      "\t a: [4, 599, 0.60828125]\n",
      "\t l: [4, 799, 2.2465968322753906]\n",
      "\t a: [4, 799, 0.627890625]\n",
      "\t Validation\n",
      "\t l: [4, 156, 1.7072654724121095]\n",
      "\t a: [4, 156, 0.5165625]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 2.154655303955078]\n",
      "\t a: [5, 199, 0.660078125]\n",
      "\t l: [5, 399, 2.156971130371094]\n",
      "\t a: [5, 399, 0.656484375]\n",
      "\t l: [5, 599, 2.1511795043945314]\n",
      "\t a: [5, 599, 0.655234375]\n",
      "\t l: [5, 799, 2.11774658203125]\n",
      "\t a: [5, 799, 0.67265625]\n",
      "\t Validation\n",
      "\t l: [5, 156, 1.6091082763671876]\n",
      "\t a: [5, 156, 0.546796875]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 2.033799133300781]\n",
      "\t a: [6, 199, 0.696640625]\n",
      "\t l: [6, 399, 2.0421211242675783]\n",
      "\t a: [6, 399, 0.692109375]\n",
      "\t l: [6, 599, 2.0404397583007814]\n",
      "\t a: [6, 599, 0.689765625]\n",
      "\t l: [6, 799, 2.010645751953125]\n",
      "\t a: [6, 799, 0.703515625]\n",
      "\t Validation\n",
      "\t l: [6, 156, 1.528914794921875]\n",
      "\t a: [6, 156, 0.567890625]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.9352459716796875]\n",
      "\t a: [7, 199, 0.7225]\n",
      "\t l: [7, 399, 1.9466387939453125]\n",
      "\t a: [7, 399, 0.7175]\n",
      "\t l: [7, 599, 1.9470094299316407]\n",
      "\t a: [7, 599, 0.715546875]\n",
      "\t l: [7, 799, 1.919530029296875]\n",
      "\t a: [7, 799, 0.724765625]\n",
      "\t Validation\n",
      "\t l: [7, 156, 1.4600701904296876]\n",
      "\t a: [7, 156, 0.58359375]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.8504151916503906]\n",
      "\t a: [8, 199, 0.741796875]\n",
      "\t l: [8, 399, 1.8659669494628905]\n",
      "\t a: [8, 399, 0.735234375]\n",
      "\t l: [8, 599, 1.8694451904296876]\n",
      "\t a: [8, 599, 0.734375]\n",
      "\t l: [8, 799, 1.8447804260253906]\n",
      "\t a: [8, 799, 0.7415625]\n",
      "\t Validation\n",
      "\t l: [8, 156, 1.4028421020507813]\n",
      "\t a: [8, 156, 0.59703125]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.7797195434570312]\n",
      "\t a: [9, 199, 0.75828125]\n",
      "\t l: [9, 399, 1.7978425598144532]\n",
      "\t a: [9, 399, 0.747890625]\n",
      "\t l: [9, 599, 1.80298583984375]\n",
      "\t a: [9, 599, 0.74796875]\n",
      "\t l: [9, 799, 1.7797215270996094]\n",
      "\t a: [9, 799, 0.75484375]\n",
      "\t Validation\n",
      "\t l: [9, 156, 1.3516410827636718]\n",
      "\t a: [9, 156, 0.6065625]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "m = LogisticRegressor()\n",
    "values_5 = train_loop(m,train_generator, test_generator, logisticLoss, 10, m5_fwd_train_batch, 1e-4, 200)\n",
    "with open(\"../v5.pk\",'wb') as f:\n",
    "    pickle.dump(values_5, f)\n",
    "m = LogisticRegressor()\n",
    "\n",
    "\n",
    "values_10 = train_loop(m,train_generator, test_generator, logisticLoss, 10, m10_fwd_train_batch, 1e-4, 200)\n",
    "with open(\"../v10.pk\",'wb') as f:\n",
    "    pickle.dump(values_10, f)\n",
    "\n",
    "\n",
    "m = LogisticRegressor()\n",
    "values_20 = train_loop(m,train_generator, test_generator, logisticLoss, 10, m20_fwd_train_batch, 1e-4, 200)\n",
    "with open(\"../v20.pk\",'wb') as f:\n",
    "    pickle.dump(values_20, f)\n",
    "\n",
    "\n",
    "m = LogisticRegressor()\n",
    "values_40 = train_loop(m,train_generator, test_generator, logisticLoss, 10, m40_fwd_train_batch, 1e-4, 200)\n",
    "with open(\"../v40.pk\",'wb') as f:\n",
    "    pickle.dump(values_40, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 4.500812683105469]\n",
      "\t a: [0, 199, 0.09208335876464843]\n",
      "\t l: [0, 399, 3.4351815795898437]\n",
      "\t a: [0, 399, 0.13187503814697266]\n",
      "\t l: [0, 599, 3.3024258422851562]\n",
      "\t a: [0, 599, 0.16958337783813476]\n",
      "\t l: [0, 799, 3.2205859375]\n",
      "\t a: [0, 799, 0.201875]\n",
      "\t l: [0, 999, 3.143376770019531]\n",
      "\t a: [0, 999, 0.241875057220459]\n",
      "\t l: [0, 1199, 3.0808786010742186]\n",
      "\t a: [0, 1199, 0.26187503814697266]\n",
      "\t l: [0, 1399, 3.0037481689453127]\n",
      "\t a: [0, 1399, 0.28687492370605466]\n",
      "\t l: [0, 1599, 2.9395986938476564]\n",
      "\t a: [0, 1599, 0.32645828247070313]\n",
      "\t l: [0, 1799, 2.8847918701171875]\n",
      "\t a: [0, 1799, 0.36270835876464846]\n",
      "\t l: [0, 1999, 2.8249322509765626]\n",
      "\t a: [0, 1999, 0.37375003814697266]\n",
      "\t l: [0, 2199, 2.7455023193359374]\n",
      "\t a: [0, 2199, 0.4131249237060547]\n",
      "\t l: [0, 2399, 2.684312744140625]\n",
      "\t a: [0, 2399, 0.45645843505859374]\n",
      "\t Validation\n",
      "\t l: [0, 416, 5.46547119140625]\n",
      "\t a: [0, 416, 0.9997917938232422]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.6077838134765625]\n",
      "\t a: [1, 199, 0.48291667938232424]\n",
      "\t l: [1, 399, 2.5550469970703125]\n",
      "\t a: [1, 399, 0.503333511352539]\n",
      "\t l: [1, 599, 2.51281005859375]\n",
      "\t a: [1, 599, 0.5110416793823243]\n",
      "\t l: [1, 799, 2.510090789794922]\n",
      "\t a: [1, 799, 0.5131250762939453]\n",
      "\t l: [1, 999, 2.4273222351074217]\n",
      "\t a: [1, 999, 0.5483334732055664]\n",
      "\t l: [1, 1199, 2.4008692932128906]\n",
      "\t a: [1, 1199, 0.5524999618530273]\n",
      "\t l: [1, 1399, 2.406011047363281]\n",
      "\t a: [1, 1399, 0.5585414886474609]\n",
      "\t l: [1, 1599, 2.336026458740234]\n",
      "\t a: [1, 1599, 0.5785417175292968]\n",
      "\t l: [1, 1799, 2.3223617553710936]\n",
      "\t a: [1, 1799, 0.5927082061767578]\n",
      "\t l: [1, 1999, 2.281710662841797]\n",
      "\t a: [1, 1999, 0.6060416793823242]\n",
      "\t l: [1, 2199, 2.2320367431640626]\n",
      "\t a: [1, 2199, 0.6206249237060547]\n",
      "\t l: [1, 2399, 2.18084716796875]\n",
      "\t a: [1, 2399, 0.6429164123535156]\n",
      "\t Validation\n",
      "\t l: [1, 416, 4.481273803710938]\n",
      "\t a: [1, 416, 1.3548951721191407]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.1458544921875]\n",
      "\t a: [2, 199, 0.6508333587646484]\n",
      "\t l: [2, 399, 2.1232005310058595]\n",
      "\t a: [2, 399, 0.655625]\n",
      "\t l: [2, 599, 2.0992694091796875]\n",
      "\t a: [2, 599, 0.655]\n",
      "\t l: [2, 799, 2.1245350646972656]\n",
      "\t a: [2, 799, 0.6433334350585938]\n",
      "\t l: [2, 999, 2.0453335571289064]\n",
      "\t a: [2, 999, 0.6718751525878907]\n",
      "\t l: [2, 1199, 2.0337983703613283]\n",
      "\t a: [2, 1199, 0.6737499237060547]\n",
      "\t l: [2, 1399, 2.0775975036621093]\n",
      "\t a: [2, 1399, 0.6606251525878907]\n",
      "\t l: [2, 1599, 2.00075439453125]\n",
      "\t a: [2, 1599, 0.6937500762939454]\n",
      "\t l: [2, 1799, 2.009443817138672]\n",
      "\t a: [2, 1799, 0.6874998474121093]\n",
      "\t l: [2, 1999, 1.9742086791992188]\n",
      "\t a: [2, 1999, 0.7016666412353516]\n",
      "\t l: [2, 2199, 1.9406051635742188]\n",
      "\t a: [2, 2199, 0.7075]\n",
      "\t l: [2, 2399, 1.8925152587890626]\n",
      "\t a: [2, 2399, 0.7331251525878906]\n",
      "\t Validation\n",
      "\t l: [2, 416, 3.914420166015625]\n",
      "\t a: [2, 416, 1.5131253051757811]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 1.87786376953125]\n",
      "\t a: [3, 199, 0.7220831298828125]\n",
      "\t l: [3, 399, 1.8708657836914062]\n",
      "\t a: [3, 399, 0.7241667938232422]\n",
      "\t l: [3, 599, 1.8555783081054686]\n",
      "\t a: [3, 599, 0.7237500762939453]\n",
      "\t l: [3, 799, 1.8911222839355468]\n",
      "\t a: [3, 799, 0.7118745422363282]\n",
      "\t l: [3, 999, 1.815628204345703]\n",
      "\t a: [3, 999, 0.7322917175292969]\n",
      "\t l: [3, 1199, 1.8107904052734376]\n",
      "\t a: [3, 1199, 0.7341665649414062]\n",
      "\t l: [3, 1399, 1.8745704650878907]\n",
      "\t a: [3, 1399, 0.7133333587646484]\n",
      "\t l: [3, 1599, 1.7916490173339843]\n",
      "\t a: [3, 1599, 0.7439582061767578]\n",
      "\t l: [3, 1799, 1.8135328674316407]\n",
      "\t a: [3, 1799, 0.7316665649414062]\n",
      "\t l: [3, 1999, 1.7794773864746094]\n",
      "\t a: [3, 1999, 0.7497917175292969]\n",
      "\t l: [3, 2199, 1.7551567077636718]\n",
      "\t a: [3, 2199, 0.7483334350585937]\n",
      "\t l: [3, 2399, 1.7077427673339844]\n",
      "\t a: [3, 2399, 0.7720834350585938]\n",
      "\t Validation\n",
      "\t l: [3, 416, 3.548054504394531]\n",
      "\t a: [3, 416, 1.5942695617675782]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 1.7038388061523437]\n",
      "\t a: [4, 199, 0.7633332824707031]\n",
      "\t l: [4, 399, 1.7057402038574219]\n",
      "\t a: [4, 399, 0.7622917938232422]\n",
      "\t l: [4, 599, 1.6955206298828125]\n",
      "\t a: [4, 599, 0.7595834350585937]\n",
      "\t l: [4, 799, 1.7347222900390624]\n",
      "\t a: [4, 799, 0.746666488647461]\n",
      "\t l: [4, 999, 1.662205810546875]\n",
      "\t a: [4, 999, 0.7658332824707031]\n",
      "\t l: [4, 1199, 1.6608889770507813]\n",
      "\t a: [4, 1199, 0.7681248474121094]\n",
      "\t l: [4, 1399, 1.7361488342285156]\n",
      "\t a: [4, 1399, 0.7402083587646484]\n",
      "\t l: [4, 1599, 1.6484506225585938]\n",
      "\t a: [4, 1599, 0.7729164123535156]\n",
      "\t l: [4, 1799, 1.6788372802734375]\n",
      "\t a: [4, 1799, 0.7625000762939453]\n",
      "\t l: [4, 1999, 1.6447512817382812]\n",
      "\t a: [4, 1999, 0.7772917175292968]\n",
      "\t l: [4, 2199, 1.6262835693359374]\n",
      "\t a: [4, 2199, 0.7754164886474609]\n",
      "\t l: [4, 2399, 1.5786888122558593]\n",
      "\t a: [4, 2399, 0.7941667175292969]\n",
      "\t Validation\n",
      "\t l: [4, 416, 3.290048828125]\n",
      "\t a: [4, 416, 1.644374237060547]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 1.5809088134765625]\n",
      "\t a: [5, 199, 0.784375]\n",
      "\t l: [5, 399, 1.5883847045898438]\n",
      "\t a: [5, 399, 0.784375]\n",
      "\t l: [5, 599, 1.5816307067871094]\n",
      "\t a: [5, 599, 0.7810417175292969]\n",
      "\t l: [5, 799, 1.6218104553222656]\n",
      "\t a: [5, 799, 0.7702082824707032]\n",
      "\t l: [5, 999, 1.5516354370117187]\n",
      "\t a: [5, 999, 0.7870834350585938]\n",
      "\t l: [5, 1199, 1.5524620056152343]\n",
      "\t a: [5, 1199, 0.7885413360595703]\n",
      "\t l: [5, 1399, 1.6348240661621094]\n",
      "\t a: [5, 1399, 0.7585417175292969]\n",
      "\t l: [5, 1599, 1.5435040283203125]\n",
      "\t a: [5, 1599, 0.7947915649414062]\n",
      "\t l: [5, 1799, 1.579716339111328]\n",
      "\t a: [5, 1799, 0.7787503051757813]\n",
      "\t l: [5, 1999, 1.5453341674804688]\n",
      "\t a: [5, 1999, 0.7943750762939453]\n",
      "\t l: [5, 2199, 1.530821533203125]\n",
      "\t a: [5, 2199, 0.7922915649414063]\n",
      "\t l: [5, 2399, 1.4827467346191405]\n",
      "\t a: [5, 2399, 0.8104167175292969]\n",
      "\t Validation\n",
      "\t l: [5, 416, 3.096848449707031]\n",
      "\t a: [5, 416, 1.679479217529297]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 1.4886460876464844]\n",
      "\t a: [6, 199, 0.8031246185302734]\n",
      "\t l: [6, 399, 1.499911346435547]\n",
      "\t a: [6, 399, 0.7970832061767578]\n",
      "\t l: [6, 599, 1.4957647705078125]\n",
      "\t a: [6, 599, 0.7985418701171875]\n",
      "\t l: [6, 799, 1.535767822265625]\n",
      "\t a: [6, 799, 0.7872914886474609]\n",
      "\t l: [6, 999, 1.4674749755859375]\n",
      "\t a: [6, 999, 0.8004167175292969]\n",
      "\t l: [6, 1199, 1.4697640991210938]\n",
      "\t a: [6, 1199, 0.8008332061767578]\n",
      "\t l: [6, 1399, 1.556746063232422]\n",
      "\t a: [6, 1399, 0.7720831298828125]\n",
      "\t l: [6, 1599, 1.4627044677734375]\n",
      "\t a: [6, 1599, 0.8085415649414063]\n",
      "\t l: [6, 1799, 1.5030828857421874]\n",
      "\t a: [6, 1799, 0.7908335113525391]\n",
      "\t l: [6, 1999, 1.4684291076660156]\n",
      "\t a: [6, 1999, 0.8064582061767578]\n",
      "\t l: [6, 2199, 1.4567306518554688]\n",
      "\t a: [6, 2199, 0.8012499237060546]\n",
      "\t l: [6, 2399, 1.4080868530273438]\n",
      "\t a: [6, 2399, 0.8231250762939453]\n",
      "\t Validation\n",
      "\t l: [6, 416, 2.9455877685546876]\n",
      "\t a: [6, 416, 1.7015621948242188]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.416270751953125]\n",
      "\t a: [7, 199, 0.8127079772949218]\n",
      "\t l: [7, 399, 1.4302882385253906]\n",
      "\t a: [7, 399, 0.8085414123535156]\n",
      "\t l: [7, 599, 1.4282196044921875]\n",
      "\t a: [7, 599, 0.8081250762939454]\n",
      "\t l: [7, 799, 1.4675309753417969]\n",
      "\t a: [7, 799, 0.8004164123535156]\n",
      "\t l: [7, 999, 1.4008018493652343]\n",
      "\t a: [7, 999, 0.808958511352539]\n",
      "\t l: [7, 1199, 1.404171905517578]\n",
      "\t a: [7, 1199, 0.8137499237060547]\n",
      "\t l: [7, 1399, 1.4942568969726562]\n",
      "\t a: [7, 1399, 0.7833329010009765]\n",
      "\t l: [7, 1599, 1.3981745910644532]\n",
      "\t a: [7, 1599, 0.8164582824707032]\n",
      "\t l: [7, 1799, 1.441626739501953]\n",
      "\t a: [7, 1799, 0.8025]\n",
      "\t l: [7, 1999, 1.4068003845214845]\n",
      "\t a: [7, 1999, 0.8158330535888672]\n",
      "\t l: [7, 2199, 1.39718505859375]\n",
      "\t a: [7, 2199, 0.810625]\n",
      "\t l: [7, 2399, 1.3479701232910157]\n",
      "\t a: [7, 2399, 0.8318751525878906]\n",
      "\t Validation\n",
      "\t l: [7, 416, 2.823156433105469]\n",
      "\t a: [7, 416, 1.720833740234375]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.3575906372070312]\n",
      "\t a: [8, 199, 0.8216664123535157]\n",
      "\t l: [8, 399, 1.3737174987792968]\n",
      "\t a: [8, 399, 0.8160414123535156]\n",
      "\t l: [8, 599, 1.3733644104003906]\n",
      "\t a: [8, 599, 0.8164585113525391]\n",
      "\t l: [8, 799, 1.411754150390625]\n",
      "\t a: [8, 799, 0.8072911834716797]\n",
      "\t l: [8, 999, 1.3463697814941407]\n",
      "\t a: [8, 999, 0.8179164886474609]\n",
      "\t l: [8, 1199, 1.3505804443359375]\n",
      "\t a: [8, 1199, 0.8214582824707031]\n",
      "\t l: [8, 1399, 1.4427902221679687]\n",
      "\t a: [8, 1399, 0.7918747711181641]\n",
      "\t l: [8, 1599, 1.345177001953125]\n",
      "\t a: [8, 1599, 0.8233333587646484]\n",
      "\t l: [8, 1799, 1.390948028564453]\n",
      "\t a: [8, 1799, 0.8097915649414062]\n",
      "\t l: [8, 1999, 1.3560536193847657]\n",
      "\t a: [8, 1999, 0.8195830535888672]\n",
      "\t l: [8, 2199, 1.3480316162109376]\n",
      "\t a: [8, 2199, 0.8172918701171875]\n",
      "\t l: [8, 2399, 1.298274688720703]\n",
      "\t a: [8, 2399, 0.8385417175292968]\n",
      "\t Validation\n",
      "\t l: [8, 416, 2.7215090942382814]\n",
      "\t a: [8, 416, 1.7352088928222655]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.308793487548828]\n",
      "\t a: [9, 199, 0.8297913360595703]\n",
      "\t l: [9, 399, 1.3266104125976563]\n",
      "\t a: [9, 399, 0.8216664123535157]\n",
      "\t l: [9, 599, 1.3277064514160157]\n",
      "\t a: [9, 599, 0.8227082824707032]\n",
      "\t l: [9, 799, 1.365081787109375]\n",
      "\t a: [9, 799, 0.8133331298828125]\n",
      "\t l: [9, 999, 1.3008827209472655]\n",
      "\t a: [9, 999, 0.8237498474121093]\n",
      "\t l: [9, 1199, 1.3057704162597656]\n",
      "\t a: [9, 1199, 0.8283335113525391]\n",
      "\t l: [9, 1399, 1.399448699951172]\n",
      "\t a: [9, 1399, 0.7974999237060547]\n",
      "\t l: [9, 1599, 1.3006906127929687]\n",
      "\t a: [9, 1599, 0.8287500762939453]\n",
      "\t l: [9, 1799, 1.3482400512695312]\n",
      "\t a: [9, 1799, 0.8172917175292969]\n",
      "\t l: [9, 1999, 1.3133668518066406]\n",
      "\t a: [9, 1999, 0.8220830535888672]\n",
      "\t l: [9, 2199, 1.306593017578125]\n",
      "\t a: [9, 2199, 0.8227083587646484]\n",
      "\t l: [9, 2399, 1.2563365936279296]\n",
      "\t a: [9, 2399, 0.843125]\n",
      "\t Validation\n",
      "\t l: [9, 416, 2.635406188964844]\n",
      "\t a: [9, 416, 1.7437510681152344]\n",
      "--- Epoch : 10 ---\n",
      "\t Training\n",
      "\t l: [10, 199, 1.2673985290527343]\n",
      "\t a: [10, 199, 0.8349995422363281]\n",
      "\t l: [10, 399, 1.286617431640625]\n",
      "\t a: [10, 399, 0.8270832061767578]\n",
      "\t l: [10, 599, 1.2889588928222657]\n",
      "\t a: [10, 599, 0.825416488647461]\n",
      "\t l: [10, 799, 1.32529541015625]\n",
      "\t a: [10, 799, 0.818749771118164]\n",
      "\t l: [10, 999, 1.2621637725830077]\n",
      "\t a: [10, 999, 0.8291664886474609]\n",
      "\t l: [10, 1199, 1.267608413696289]\n",
      "\t a: [10, 1199, 0.8327084350585937]\n",
      "\t l: [10, 1399, 1.3622991943359375]\n",
      "\t a: [10, 1399, 0.8047917175292969]\n",
      "\t l: [10, 1599, 1.262691650390625]\n",
      "\t a: [10, 1599, 0.8331252288818359]\n",
      "\t l: [10, 1799, 1.31162109375]\n",
      "\t a: [10, 1799, 0.8220832824707032]\n",
      "\t l: [10, 1999, 1.2768372344970702]\n",
      "\t a: [10, 1999, 0.8268748474121094]\n",
      "\t l: [10, 2199, 1.2710661315917968]\n",
      "\t a: [10, 2199, 0.8268748474121094]\n",
      "\t l: [10, 2399, 1.2203516387939453]\n",
      "\t a: [10, 2399, 0.8477082824707032]\n",
      "\t Validation\n",
      "\t l: [10, 416, 2.561293640136719]\n",
      "\t a: [10, 416, 1.7535427856445311]\n",
      "--- Epoch : 11 ---\n",
      "\t Training\n",
      "\t l: [11, 199, 1.2317179870605468]\n",
      "\t a: [11, 199, 0.840207748413086]\n",
      "\t l: [11, 399, 1.2521326446533203]\n",
      "\t a: [11, 399, 0.8308332061767578]\n",
      "\t l: [11, 599, 1.2555580139160156]\n",
      "\t a: [11, 599, 0.8297916412353515]\n",
      "\t l: [11, 799, 1.2908627319335937]\n",
      "\t a: [11, 799, 0.8237495422363281]\n",
      "\t l: [11, 999, 1.228709716796875]\n",
      "\t a: [11, 999, 0.835]\n",
      "\t l: [11, 1199, 1.2346199035644532]\n",
      "\t a: [11, 1199, 0.8387501525878907]\n",
      "\t l: [11, 1399, 1.3299986267089843]\n",
      "\t a: [11, 1399, 0.8102083587646485]\n",
      "\t l: [11, 1599, 1.22976806640625]\n",
      "\t a: [11, 1599, 0.836875228881836]\n",
      "\t l: [11, 1799, 1.2797774505615234]\n",
      "\t a: [11, 1799, 0.824791488647461]\n",
      "\t l: [11, 1999, 1.2451361846923827]\n",
      "\t a: [11, 1999, 0.8299999237060547]\n",
      "\t l: [11, 2199, 1.2401831817626954]\n",
      "\t a: [11, 2199, 0.8310416412353515]\n",
      "\t l: [11, 2399, 1.189052200317383]\n",
      "\t a: [11, 2399, 0.8529165649414062]\n",
      "\t Validation\n",
      "\t l: [11, 416, 2.4966525268554687]\n",
      "\t a: [11, 416, 1.7616679382324218]\n",
      "--- Epoch : 12 ---\n",
      "\t Training\n",
      "\t l: [12, 199, 1.200557632446289]\n",
      "\t a: [12, 199, 0.8454161071777344]\n",
      "\t l: [12, 399, 1.222015609741211]\n",
      "\t a: [12, 399, 0.8368748474121094]\n",
      "\t l: [12, 599, 1.226395034790039]\n",
      "\t a: [12, 599, 0.8325]\n",
      "\t l: [12, 799, 1.2606890106201172]\n",
      "\t a: [12, 799, 0.8287496948242188]\n",
      "\t l: [12, 999, 1.1994453430175782]\n",
      "\t a: [12, 999, 0.8385415649414063]\n",
      "\t l: [12, 1199, 1.2057481384277344]\n",
      "\t a: [12, 1199, 0.840833740234375]\n",
      "\t l: [12, 1399, 1.3015802001953125]\n",
      "\t a: [12, 1399, 0.8152084350585938]\n",
      "\t l: [12, 1599, 1.2009029388427734]\n",
      "\t a: [12, 1599, 0.8406251525878906]\n",
      "\t l: [12, 1799, 1.251763153076172]\n",
      "\t a: [12, 1799, 0.82875]\n",
      "\t l: [12, 1999, 1.2172992706298829]\n",
      "\t a: [12, 1999, 0.8339582061767579]\n",
      "\t l: [12, 2199, 1.2130271148681642]\n",
      "\t a: [12, 2199, 0.8354166412353515]\n",
      "\t l: [12, 2399, 1.1615162658691407]\n",
      "\t a: [12, 2399, 0.8558332061767578]\n",
      "\t Validation\n",
      "\t l: [12, 416, 2.4396522521972654]\n",
      "\t a: [12, 416, 1.7693763732910157]\n",
      "--- Epoch : 13 ---\n",
      "\t Training\n",
      "\t l: [13, 199, 1.1730464935302733]\n",
      "\t a: [13, 199, 0.8499994659423828]\n",
      "\t l: [13, 399, 1.1954311370849608]\n",
      "\t a: [13, 399, 0.840625]\n",
      "\t l: [13, 599, 1.2006573486328125]\n",
      "\t a: [13, 599, 0.8356248474121094]\n",
      "\t l: [13, 799, 1.2339720916748047]\n",
      "\t a: [13, 799, 0.8314579772949219]\n",
      "\t l: [13, 999, 1.1735804748535157]\n",
      "\t a: [13, 999, 0.8441664886474609]\n",
      "\t l: [13, 1199, 1.180217056274414]\n",
      "\t a: [13, 1199, 0.8433335876464844]\n",
      "\t l: [13, 1399, 1.2763300323486328]\n",
      "\t a: [13, 1399, 0.8183335876464843]\n",
      "\t l: [13, 1599, 1.175343017578125]\n",
      "\t a: [13, 1599, 0.8445835113525391]\n",
      "\t l: [13, 1799, 1.226873321533203]\n",
      "\t a: [13, 1799, 0.8320831298828125]\n",
      "\t l: [13, 1999, 1.1926146697998048]\n",
      "\t a: [13, 1999, 0.8395832824707031]\n",
      "\t l: [13, 2199, 1.1889152526855469]\n",
      "\t a: [13, 2199, 0.8381251525878907]\n",
      "\t l: [13, 2399, 1.137058639526367]\n",
      "\t a: [13, 2399, 0.8579164123535157]\n",
      "\t Validation\n",
      "\t l: [13, 416, 2.3889208984375]\n",
      "\t a: [13, 416, 1.7739596557617188]\n",
      "--- Epoch : 14 ---\n",
      "\t Training\n",
      "\t l: [14, 199, 1.1485314178466797]\n",
      "\t a: [14, 199, 0.8518745422363281]\n",
      "\t l: [14, 399, 1.1717517852783204]\n",
      "\t a: [14, 399, 0.845624771118164]\n",
      "\t l: [14, 599, 1.1777352142333983]\n",
      "\t a: [14, 599, 0.8381247711181641]\n",
      "\t l: [14, 799, 1.2101038360595704]\n",
      "\t a: [14, 799, 0.8339579772949218]\n",
      "\t l: [14, 999, 1.1505177307128907]\n",
      "\t a: [14, 999, 0.8452081298828125]\n",
      "\t l: [14, 1199, 1.1574393463134767]\n",
      "\t a: [14, 1199, 0.8460418701171875]\n",
      "\t l: [14, 1399, 1.25370361328125]\n",
      "\t a: [14, 1399, 0.8214585876464844]\n",
      "\t l: [14, 1599, 1.152514419555664]\n",
      "\t a: [14, 1599, 0.846250228881836]\n",
      "\t l: [14, 1799, 1.2045758056640625]\n",
      "\t a: [14, 1799, 0.836041488647461]\n",
      "\t l: [14, 1999, 1.1705377197265625]\n",
      "\t a: [14, 1999, 0.8429166412353516]\n",
      "\t l: [14, 2199, 1.1673283386230469]\n",
      "\t a: [14, 2199, 0.8410416412353515]\n",
      "\t l: [14, 2399, 1.1151556396484374]\n",
      "\t a: [14, 2399, 0.8616665649414063]\n",
      "\t Validation\n",
      "\t l: [14, 416, 2.3434078979492186]\n",
      "\t a: [14, 416, 1.7815638732910157]\n",
      "--- Epoch : 15 ---\n",
      "\t Training\n",
      "\t l: [15, 199, 1.1265142059326172]\n",
      "\t a: [15, 199, 0.8552079010009765]\n",
      "\t l: [15, 399, 1.1504965209960938]\n",
      "\t a: [15, 399, 0.8479164123535157]\n",
      "\t l: [15, 599, 1.1571623992919922]\n",
      "\t a: [15, 599, 0.8420832061767578]\n",
      "\t l: [15, 799, 1.1886186981201172]\n",
      "\t a: [15, 799, 0.8389581298828125]\n",
      "\t l: [15, 999, 1.129798049926758]\n",
      "\t a: [15, 999, 0.846249771118164]\n",
      "\t l: [15, 1199, 1.1369622802734376]\n",
      "\t a: [15, 1199, 0.8500001525878906]\n",
      "\t l: [15, 1399, 1.2332821655273438]\n",
      "\t a: [15, 1399, 0.8245836639404297]\n",
      "\t l: [15, 1599, 1.1319769287109376]\n",
      "\t a: [15, 1599, 0.8489585876464844]\n",
      "\t l: [15, 1799, 1.184455032348633]\n",
      "\t a: [15, 1799, 0.8383331298828125]\n",
      "\t l: [15, 1999, 1.1506494903564453]\n",
      "\t a: [15, 1999, 0.8460416412353515]\n",
      "\t l: [15, 2199, 1.1478622436523438]\n",
      "\t a: [15, 2199, 0.8433332824707032]\n",
      "\t l: [15, 2399, 1.0954009246826173]\n",
      "\t a: [15, 2399, 0.8643749237060547]\n",
      "\t Validation\n",
      "\t l: [15, 416, 2.3022955322265624]\n",
      "\t a: [15, 416, 1.7863551330566407]\n",
      "--- Epoch : 16 ---\n",
      "\t Training\n",
      "\t l: [16, 199, 1.1066048431396485]\n",
      "\t a: [16, 199, 0.8564579772949219]\n",
      "\t l: [16, 399, 1.1312879180908204]\n",
      "\t a: [16, 399, 0.850624771118164]\n",
      "\t l: [16, 599, 1.1385716247558593]\n",
      "\t a: [16, 599, 0.8447916412353516]\n",
      "\t l: [16, 799, 1.1691499328613282]\n",
      "\t a: [16, 799, 0.8414582061767578]\n",
      "\t l: [16, 999, 1.1110594177246094]\n",
      "\t a: [16, 999, 0.8495832824707031]\n",
      "\t l: [16, 1199, 1.118431930541992]\n",
      "\t a: [16, 1199, 0.8533334350585937]\n",
      "\t l: [16, 1399, 1.2147333526611328]\n",
      "\t a: [16, 1399, 0.8264585113525391]\n",
      "\t l: [16, 1599, 1.113380126953125]\n",
      "\t a: [16, 1599, 0.8514584350585938]\n",
      "\t l: [16, 1799, 1.1661857604980468]\n",
      "\t a: [16, 1799, 0.8408332061767578]\n",
      "\t l: [16, 1999, 1.1326177978515626]\n",
      "\t a: [16, 1999, 0.8479165649414062]\n",
      "\t l: [16, 2199, 1.1301988983154296]\n",
      "\t a: [16, 2199, 0.8447915649414063]\n",
      "\t l: [16, 2399, 1.0774715423583985]\n",
      "\t a: [16, 2399, 0.8664582061767578]\n",
      "\t Validation\n",
      "\t l: [16, 416, 2.2649334716796874]\n",
      "\t a: [16, 416, 1.78885498046875]\n",
      "--- Epoch : 17 ---\n",
      "\t Training\n",
      "\t l: [17, 199, 1.0884925842285156]\n",
      "\t a: [17, 199, 0.8591664123535157]\n",
      "\t l: [17, 399, 1.113826141357422]\n",
      "\t a: [17, 399, 0.8537497711181641]\n",
      "\t l: [17, 599, 1.1216734313964845]\n",
      "\t a: [17, 599, 0.8468747711181641]\n",
      "\t l: [17, 799, 1.1514048767089844]\n",
      "\t a: [17, 799, 0.844375]\n",
      "\t l: [17, 999, 1.0940138244628905]\n",
      "\t a: [17, 999, 0.8520832824707031]\n",
      "\t l: [17, 1199, 1.1015642547607423]\n",
      "\t a: [17, 1199, 0.8552084350585938]\n",
      "\t l: [17, 1399, 1.1977924346923827]\n",
      "\t a: [17, 1399, 0.8293752288818359]\n",
      "\t l: [17, 1599, 1.0964456939697265]\n",
      "\t a: [17, 1599, 0.8527085113525391]\n",
      "\t l: [17, 1799, 1.1495042419433594]\n",
      "\t a: [17, 1799, 0.8422915649414062]\n",
      "\t l: [17, 1999, 1.1161774444580077]\n",
      "\t a: [17, 1999, 0.8495831298828125]\n",
      "\t l: [17, 2199, 1.1140824890136718]\n",
      "\t a: [17, 2199, 0.8468749237060547]\n",
      "\t l: [17, 2399, 1.0611094665527343]\n",
      "\t a: [17, 2399, 0.867916488647461]\n",
      "\t Validation\n",
      "\t l: [17, 416, 2.2307981872558593]\n",
      "\t a: [17, 416, 1.7923966979980468]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 5.740050048828125]\n",
      "\t a: [0, 199, 0.0710416841506958]\n",
      "\t l: [0, 399, 3.4603656005859373]\n",
      "\t a: [0, 399, 0.11895834922790527]\n",
      "\t l: [0, 599, 3.2946795654296874]\n",
      "\t a: [0, 599, 0.16166664123535157]\n",
      "\t l: [0, 799, 3.1963055419921873]\n",
      "\t a: [0, 799, 0.196875]\n",
      "\t l: [0, 999, 3.1531280517578124]\n",
      "\t a: [0, 999, 0.23375001907348633]\n",
      "\t l: [0, 1199, 3.050367431640625]\n",
      "\t a: [0, 1199, 0.2827083396911621]\n",
      "\t l: [0, 1399, 3.006248779296875]\n",
      "\t a: [0, 1399, 0.29562496185302733]\n",
      "\t l: [0, 1599, 2.9137518310546877]\n",
      "\t a: [0, 1599, 0.3308333206176758]\n",
      "\t l: [0, 1799, 2.8773153686523436]\n",
      "\t a: [0, 1799, 0.3462499237060547]\n",
      "\t l: [0, 1999, 2.847875061035156]\n",
      "\t a: [0, 1999, 0.36187503814697264]\n",
      "\t l: [0, 2199, 2.7798092651367186]\n",
      "\t a: [0, 2199, 0.384375]\n",
      "\t l: [0, 2399, 2.732181396484375]\n",
      "\t a: [0, 2399, 0.4041665267944336]\n",
      "\t Validation\n",
      "\t l: [0, 416, 5.559374389648437]\n",
      "\t a: [0, 416, 0.8998957824707031]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.6839779663085936]\n",
      "\t a: [1, 199, 0.43437503814697265]\n",
      "\t l: [1, 399, 2.650643005371094]\n",
      "\t a: [1, 399, 0.44583347320556643]\n",
      "\t l: [1, 599, 2.615744934082031]\n",
      "\t a: [1, 599, 0.45395843505859373]\n",
      "\t l: [1, 799, 2.5800442504882812]\n",
      "\t a: [1, 799, 0.4797916030883789]\n",
      "\t l: [1, 999, 2.5109765625]\n",
      "\t a: [1, 999, 0.4975]\n",
      "\t l: [1, 1199, 2.4713558959960937]\n",
      "\t a: [1, 1199, 0.5143749237060546]\n",
      "\t l: [1, 1399, 2.5034970092773436]\n",
      "\t a: [1, 1399, 0.4985417556762695]\n",
      "\t l: [1, 1599, 2.3982783508300782]\n",
      "\t a: [1, 1599, 0.5433333587646484]\n",
      "\t l: [1, 1799, 2.3982745361328126]\n",
      "\t a: [1, 1799, 0.5408333969116211]\n",
      "\t l: [1, 1999, 2.357766571044922]\n",
      "\t a: [1, 1999, 0.5504166793823242]\n",
      "\t l: [1, 2199, 2.3253933715820314]\n",
      "\t a: [1, 2199, 0.5658333206176758]\n",
      "\t l: [1, 2399, 2.2517214965820314]\n",
      "\t a: [1, 2399, 0.5977083969116211]\n",
      "\t Validation\n",
      "\t l: [1, 416, 4.626562194824219]\n",
      "\t a: [1, 416, 1.257187271118164]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.2297645568847657]\n",
      "\t a: [2, 199, 0.6122918701171876]\n",
      "\t l: [2, 399, 2.2192948913574218]\n",
      "\t a: [2, 399, 0.6070835494995117]\n",
      "\t l: [2, 599, 2.195292510986328]\n",
      "\t a: [2, 599, 0.6072917556762696]\n",
      "\t l: [2, 799, 2.215225830078125]\n",
      "\t a: [2, 799, 0.5995834350585938]\n",
      "\t l: [2, 999, 2.1405938720703124]\n",
      "\t a: [2, 999, 0.6235417556762696]\n",
      "\t l: [2, 1199, 2.115767822265625]\n",
      "\t a: [2, 1199, 0.6193749618530273]\n",
      "\t l: [2, 1399, 2.179288024902344]\n",
      "\t a: [2, 1399, 0.6064583969116211]\n",
      "\t l: [2, 1599, 2.0656565856933593]\n",
      "\t a: [2, 1599, 0.6472914886474609]\n",
      "\t l: [2, 1799, 2.095758514404297]\n",
      "\t a: [2, 1799, 0.6402082061767578]\n",
      "\t l: [2, 1999, 2.068595275878906]\n",
      "\t a: [2, 1999, 0.6466667938232422]\n",
      "\t l: [2, 2199, 2.043861999511719]\n",
      "\t a: [2, 2199, 0.6397918319702148]\n",
      "\t l: [2, 2399, 1.999610137939453]\n",
      "\t a: [2, 2399, 0.6745835876464844]\n",
      "\t Validation\n",
      "\t l: [2, 416, 4.109828796386719]\n",
      "\t a: [2, 416, 1.3967710876464843]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 1.9833094787597656]\n",
      "\t a: [3, 199, 0.668333511352539]\n",
      "\t l: [3, 399, 1.9969813537597656]\n",
      "\t a: [3, 399, 0.6566665649414063]\n",
      "\t l: [3, 599, 1.9842741394042969]\n",
      "\t a: [3, 599, 0.6662500762939453]\n",
      "\t l: [3, 799, 2.0097511291503904]\n",
      "\t a: [3, 799, 0.6522916412353515]\n",
      "\t l: [3, 999, 1.933834228515625]\n",
      "\t a: [3, 999, 0.6708334350585937]\n",
      "\t l: [3, 1199, 1.9256304931640624]\n",
      "\t a: [3, 1199, 0.6731248474121094]\n",
      "\t l: [3, 1399, 2.0166944885253906]\n",
      "\t a: [3, 1399, 0.6456251525878907]\n",
      "\t l: [3, 1599, 1.8871920776367188]\n",
      "\t a: [3, 1599, 0.6954166412353515]\n",
      "\t l: [3, 1799, 1.9228514099121095]\n",
      "\t a: [3, 1799, 0.6775000762939453]\n",
      "\t l: [3, 1999, 1.9061384582519532]\n",
      "\t a: [3, 1999, 0.6827082824707031]\n",
      "\t l: [3, 2199, 1.8739154052734375]\n",
      "\t a: [3, 2199, 0.6872919464111328]\n",
      "\t l: [3, 2399, 1.8217849731445312]\n",
      "\t a: [3, 2399, 0.7099999237060547]\n",
      "\t Validation\n",
      "\t l: [3, 416, 3.7790374755859375]\n",
      "\t a: [3, 416, 1.4613548278808595]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 1.8308808898925781]\n",
      "\t a: [4, 199, 0.6920832824707032]\n",
      "\t l: [4, 399, 1.8414480590820312]\n",
      "\t a: [4, 399, 0.685625]\n",
      "\t l: [4, 599, 1.8372348022460938]\n",
      "\t a: [4, 599, 0.6939582061767579]\n",
      "\t l: [4, 799, 1.8550631713867187]\n",
      "\t a: [4, 799, 0.6806253051757812]\n",
      "\t l: [4, 999, 1.7747659301757812]\n",
      "\t a: [4, 999, 0.7033335113525391]\n",
      "\t l: [4, 1199, 1.7575462341308594]\n",
      "\t a: [4, 1199, 0.7110419464111328]\n",
      "\t l: [4, 1399, 1.8488021850585938]\n",
      "\t a: [4, 1399, 0.6822917938232422]\n",
      "\t l: [4, 1599, 1.7320529174804689]\n",
      "\t a: [4, 1599, 0.7187499237060547]\n",
      "\t l: [4, 1799, 1.772127685546875]\n",
      "\t a: [4, 1799, 0.7074999237060546]\n",
      "\t l: [4, 1999, 1.7546954345703125]\n",
      "\t a: [4, 1999, 0.7091664886474609]\n",
      "\t l: [4, 2199, 1.7209565734863281]\n",
      "\t a: [4, 2199, 0.7179167175292969]\n",
      "\t l: [4, 2399, 1.6644697570800782]\n",
      "\t a: [4, 2399, 0.7441666412353516]\n",
      "\t Validation\n",
      "\t l: [4, 416, 3.4657254028320312]\n",
      "\t a: [4, 416, 1.5376039123535157]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 1.6697422790527343]\n",
      "\t a: [5, 199, 0.7406249237060547]\n",
      "\t l: [5, 399, 1.6829110717773437]\n",
      "\t a: [5, 399, 0.7252084350585938]\n",
      "\t l: [5, 599, 1.6799325561523437]\n",
      "\t a: [5, 599, 0.7270831298828125]\n",
      "\t l: [5, 799, 1.71270263671875]\n",
      "\t a: [5, 799, 0.7175000762939453]\n",
      "\t l: [5, 999, 1.639322509765625]\n",
      "\t a: [5, 999, 0.7331250762939453]\n",
      "\t l: [5, 1199, 1.6232791137695313]\n",
      "\t a: [5, 1199, 0.7399999237060547]\n",
      "\t l: [5, 1399, 1.7307107543945313]\n",
      "\t a: [5, 1399, 0.7083334350585937]\n",
      "\t l: [5, 1599, 1.6082595825195312]\n",
      "\t a: [5, 1599, 0.7483335876464844]\n",
      "\t l: [5, 1799, 1.6631785583496095]\n",
      "\t a: [5, 1799, 0.7235416412353516]\n",
      "\t l: [5, 1999, 1.6403713989257813]\n",
      "\t a: [5, 1999, 0.7341667938232422]\n",
      "\t l: [5, 2199, 1.6070634460449218]\n",
      "\t a: [5, 2199, 0.7329167175292969]\n",
      "\t l: [5, 2399, 1.5527859497070313]\n",
      "\t a: [5, 2399, 0.7629165649414062]\n",
      "\t Validation\n",
      "\t l: [5, 416, 3.247450256347656]\n",
      "\t a: [5, 416, 1.5688542175292968]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 1.5663372802734374]\n",
      "\t a: [6, 199, 0.7562502288818359]\n",
      "\t l: [6, 399, 1.585923309326172]\n",
      "\t a: [6, 399, 0.7443750762939453]\n",
      "\t l: [6, 599, 1.5952085876464843]\n",
      "\t a: [6, 599, 0.7416667938232422]\n",
      "\t l: [6, 799, 1.6202516174316406]\n",
      "\t a: [6, 799, 0.735]\n",
      "\t l: [6, 999, 1.5572816467285155]\n",
      "\t a: [6, 999, 0.7414583587646484]\n",
      "\t l: [6, 1199, 1.5436465454101562]\n",
      "\t a: [6, 1199, 0.7499999237060547]\n",
      "\t l: [6, 1399, 1.6544924926757814]\n",
      "\t a: [6, 1399, 0.7183332061767578]\n",
      "\t l: [6, 1599, 1.5355522155761718]\n",
      "\t a: [6, 1599, 0.7593749237060546]\n",
      "\t l: [6, 1799, 1.5879263305664062]\n",
      "\t a: [6, 1799, 0.7439580535888672]\n",
      "\t l: [6, 1999, 1.5549447631835938]\n",
      "\t a: [6, 1999, 0.7602084350585937]\n",
      "\t l: [6, 2199, 1.5313058471679688]\n",
      "\t a: [6, 2199, 0.7504165649414063]\n",
      "\t l: [6, 2399, 1.4741275024414062]\n",
      "\t a: [6, 2399, 0.780625]\n",
      "\t Validation\n",
      "\t l: [6, 416, 3.08867431640625]\n",
      "\t a: [6, 416, 1.6210421752929687]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.493181915283203]\n",
      "\t a: [7, 199, 0.7712501525878906]\n",
      "\t l: [7, 399, 1.5129283142089844]\n",
      "\t a: [7, 399, 0.7589583587646485]\n",
      "\t l: [7, 599, 1.5279306030273438]\n",
      "\t a: [7, 599, 0.7522916412353515]\n",
      "\t l: [7, 799, 1.5479489135742188]\n",
      "\t a: [7, 799, 0.7431249237060547]\n",
      "\t l: [7, 999, 1.4788681030273438]\n",
      "\t a: [7, 999, 0.7635419464111328]\n",
      "\t l: [7, 1199, 1.4790248107910156]\n",
      "\t a: [7, 1199, 0.7614585113525391]\n",
      "\t l: [7, 1399, 1.591183624267578]\n",
      "\t a: [7, 1399, 0.7304165649414063]\n",
      "\t l: [7, 1599, 1.4676004028320313]\n",
      "\t a: [7, 1599, 0.7804167175292969]\n",
      "\t l: [7, 1799, 1.5352647399902344]\n",
      "\t a: [7, 1799, 0.7560416412353516]\n",
      "\t l: [7, 1999, 1.5070210266113282]\n",
      "\t a: [7, 1999, 0.7604166412353516]\n",
      "\t l: [7, 2199, 1.4827348327636718]\n",
      "\t a: [7, 2199, 0.7647919464111328]\n",
      "\t l: [7, 2399, 1.4287802124023437]\n",
      "\t a: [7, 2399, 0.7916665649414063]\n",
      "\t Validation\n",
      "\t l: [7, 416, 2.997680358886719]\n",
      "\t a: [7, 416, 1.6239588928222657]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.4477143859863282]\n",
      "\t a: [8, 199, 0.7745835113525391]\n",
      "\t l: [8, 399, 1.4700376892089844]\n",
      "\t a: [8, 399, 0.7697919464111328]\n",
      "\t l: [8, 599, 1.4890559387207032]\n",
      "\t a: [8, 599, 0.7579164123535156]\n",
      "\t l: [8, 799, 1.5098582458496095]\n",
      "\t a: [8, 799, 0.7535416412353516]\n",
      "\t l: [8, 999, 1.44179931640625]\n",
      "\t a: [8, 999, 0.775]\n",
      "\t l: [8, 1199, 1.4475877380371094]\n",
      "\t a: [8, 1199, 0.7745834350585937]\n",
      "\t l: [8, 1399, 1.5535882568359376]\n",
      "\t a: [8, 1399, 0.7435417938232421]\n",
      "\t l: [8, 1599, 1.4263916015625]\n",
      "\t a: [8, 1599, 0.7862501525878907]\n",
      "\t l: [8, 1799, 1.4942041015625]\n",
      "\t a: [8, 1799, 0.759166259765625]\n",
      "\t l: [8, 1999, 1.4645475769042968]\n",
      "\t a: [8, 1999, 0.7710418701171875]\n",
      "\t l: [8, 2199, 1.4442747497558595]\n",
      "\t a: [8, 2199, 0.7683332824707031]\n",
      "\t l: [8, 2399, 1.390579833984375]\n",
      "\t a: [8, 2399, 0.793541488647461]\n",
      "\t Validation\n",
      "\t l: [8, 416, 2.9244308471679688]\n",
      "\t a: [8, 416, 1.6450001525878906]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.4030148315429687]\n",
      "\t a: [9, 199, 0.783750228881836]\n",
      "\t l: [9, 399, 1.4268353271484375]\n",
      "\t a: [9, 399, 0.7764585876464843]\n",
      "\t l: [9, 599, 1.440354766845703]\n",
      "\t a: [9, 599, 0.7714583587646484]\n",
      "\t l: [9, 799, 1.4609373474121095]\n",
      "\t a: [9, 799, 0.765]\n",
      "\t l: [9, 999, 1.3968376159667968]\n",
      "\t a: [9, 999, 0.7791665649414062]\n",
      "\t l: [9, 1199, 1.3965850830078126]\n",
      "\t a: [9, 1199, 0.7837499237060547]\n",
      "\t l: [9, 1399, 1.5179090881347657]\n",
      "\t a: [9, 1399, 0.7485416412353516]\n",
      "\t l: [9, 1599, 1.390691375732422]\n",
      "\t a: [9, 1599, 0.7935415649414063]\n",
      "\t l: [9, 1799, 1.4569166564941407]\n",
      "\t a: [9, 1799, 0.7679167175292969]\n",
      "\t l: [9, 1999, 1.4242494201660156]\n",
      "\t a: [9, 1999, 0.7789584350585937]\n",
      "\t l: [9, 2199, 1.4112251281738282]\n",
      "\t a: [9, 2199, 0.7818748474121093]\n",
      "\t l: [9, 2399, 1.357723388671875]\n",
      "\t a: [9, 2399, 0.8027079772949218]\n",
      "\t Validation\n",
      "\t l: [9, 416, 2.84323974609375]\n",
      "\t a: [9, 416, 1.659375]\n",
      "--- Epoch : 10 ---\n",
      "\t Training\n",
      "\t l: [10, 199, 1.3751316833496094]\n",
      "\t a: [10, 199, 0.7883333587646484]\n",
      "\t l: [10, 399, 1.400631866455078]\n",
      "\t a: [10, 399, 0.7822917175292968]\n",
      "\t l: [10, 599, 1.4159280395507812]\n",
      "\t a: [10, 599, 0.7795832061767578]\n",
      "\t l: [10, 799, 1.4362158203125]\n",
      "\t a: [10, 799, 0.7718751525878906]\n",
      "\t l: [10, 999, 1.3729052734375]\n",
      "\t a: [10, 999, 0.7868747711181641]\n",
      "\t l: [10, 1199, 1.3797059631347657]\n",
      "\t a: [10, 1199, 0.788125]\n",
      "\t l: [10, 1399, 1.4906558227539062]\n",
      "\t a: [10, 1399, 0.7524998474121094]\n",
      "\t l: [10, 1599, 1.3599417114257812]\n",
      "\t a: [10, 1599, 0.8022914886474609]\n",
      "\t l: [10, 1799, 1.4310853576660156]\n",
      "\t a: [10, 1799, 0.7745831298828125]\n",
      "\t l: [10, 1999, 1.395069580078125]\n",
      "\t a: [10, 1999, 0.7852083587646485]\n",
      "\t l: [10, 2199, 1.3762574768066407]\n",
      "\t a: [10, 2199, 0.7902081298828125]\n",
      "\t l: [10, 2399, 1.3226937866210937]\n",
      "\t a: [10, 2399, 0.8118747711181641]\n",
      "\t Validation\n",
      "\t l: [10, 416, 2.7859136962890627]\n",
      "\t a: [10, 416, 1.6702088928222656]\n",
      "--- Epoch : 11 ---\n",
      "\t Training\n",
      "\t l: [11, 199, 1.3435455322265626]\n",
      "\t a: [11, 199, 0.7956249237060546]\n",
      "\t l: [11, 399, 1.372214813232422]\n",
      "\t a: [11, 399, 0.7893749237060547]\n",
      "\t l: [11, 599, 1.3884397888183593]\n",
      "\t a: [11, 599, 0.7827084350585938]\n",
      "\t l: [11, 799, 1.4014411926269532]\n",
      "\t a: [11, 799, 0.7806250762939453]\n",
      "\t l: [11, 999, 1.3379798889160157]\n",
      "\t a: [11, 999, 0.7958332061767578]\n",
      "\t l: [11, 1199, 1.349964599609375]\n",
      "\t a: [11, 1199, 0.7922917175292968]\n",
      "\t l: [11, 1399, 1.4622174072265626]\n",
      "\t a: [11, 1399, 0.7608334350585938]\n",
      "\t l: [11, 1599, 1.330032958984375]\n",
      "\t a: [11, 1599, 0.8093752288818359]\n",
      "\t l: [11, 1799, 1.3986073303222657]\n",
      "\t a: [11, 1799, 0.7812499237060547]\n",
      "\t l: [11, 1999, 1.3681930541992187]\n",
      "\t a: [11, 1999, 0.793333511352539]\n",
      "\t l: [11, 2199, 1.343626708984375]\n",
      "\t a: [11, 2199, 0.7920832824707031]\n",
      "\t l: [11, 2399, 1.2950309753417968]\n",
      "\t a: [11, 2399, 0.8141667175292969]\n",
      "\t Validation\n",
      "\t l: [11, 416, 2.7175332641601564]\n",
      "\t a: [11, 416, 1.6870834350585937]\n",
      "--- Epoch : 12 ---\n",
      "\t Training\n",
      "\t l: [12, 199, 1.3064945983886718]\n",
      "\t a: [12, 199, 0.8041670227050781]\n",
      "\t l: [12, 399, 1.3376919555664062]\n",
      "\t a: [12, 399, 0.8022919464111328]\n",
      "\t l: [12, 599, 1.3515980529785157]\n",
      "\t a: [12, 599, 0.7972916412353516]\n",
      "\t l: [12, 799, 1.3671762084960937]\n",
      "\t a: [12, 799, 0.788750228881836]\n",
      "\t l: [12, 999, 1.3016371154785156]\n",
      "\t a: [12, 999, 0.8033329010009765]\n",
      "\t l: [12, 1199, 1.3255276489257812]\n",
      "\t a: [12, 1199, 0.7985419464111329]\n",
      "\t l: [12, 1399, 1.4414305114746093]\n",
      "\t a: [12, 1399, 0.7666669464111329]\n",
      "\t l: [12, 1599, 1.305872802734375]\n",
      "\t a: [12, 1599, 0.8149996948242187]\n",
      "\t l: [12, 1799, 1.3711830139160157]\n",
      "\t a: [12, 1799, 0.7904167175292969]\n",
      "\t l: [12, 1999, 1.344974365234375]\n",
      "\t a: [12, 1999, 0.7970834350585938]\n",
      "\t l: [12, 2199, 1.31892822265625]\n",
      "\t a: [12, 2199, 0.7939584350585938]\n",
      "\t l: [12, 2399, 1.2692839813232422]\n",
      "\t a: [12, 2399, 0.8139583587646484]\n",
      "\t Validation\n",
      "\t l: [12, 416, 2.671434326171875]\n",
      "\t a: [12, 416, 1.6919793701171875]\n",
      "--- Epoch : 13 ---\n",
      "\t Training\n",
      "\t l: [13, 199, 1.2802009582519531]\n",
      "\t a: [13, 199, 0.8066669464111328]\n",
      "\t l: [13, 399, 1.3143278503417968]\n",
      "\t a: [13, 399, 0.8041670227050781]\n",
      "\t l: [13, 599, 1.318782958984375]\n",
      "\t a: [13, 599, 0.7970832824707031]\n",
      "\t l: [13, 799, 1.3445832824707031]\n",
      "\t a: [13, 799, 0.7939581298828124]\n",
      "\t l: [13, 999, 1.2743310546875]\n",
      "\t a: [13, 999, 0.8062496185302734]\n",
      "\t l: [13, 1199, 1.3001475524902344]\n",
      "\t a: [13, 1199, 0.8064584350585937]\n",
      "\t l: [13, 1399, 1.4061505126953124]\n",
      "\t a: [13, 1399, 0.7752084350585937]\n",
      "\t l: [13, 1599, 1.279375]\n",
      "\t a: [13, 1599, 0.8179165649414063]\n",
      "\t l: [13, 1799, 1.341396026611328]\n",
      "\t a: [13, 1799, 0.7958330535888671]\n",
      "\t l: [13, 1999, 1.3210258483886719]\n",
      "\t a: [13, 1999, 0.8012499237060546]\n",
      "\t l: [13, 2199, 1.2879647827148437]\n",
      "\t a: [13, 2199, 0.8008336639404297]\n",
      "\t l: [13, 2399, 1.2397832489013672]\n",
      "\t a: [13, 2399, 0.8204170227050781]\n",
      "\t Validation\n",
      "\t l: [13, 416, 2.615594482421875]\n",
      "\t a: [13, 416, 1.704687042236328]\n",
      "--- Epoch : 14 ---\n",
      "\t Training\n",
      "\t l: [14, 199, 1.2541940307617188]\n",
      "\t a: [14, 199, 0.8129167938232422]\n",
      "\t l: [14, 399, 1.296158447265625]\n",
      "\t a: [14, 399, 0.8043751525878906]\n",
      "\t l: [14, 599, 1.3080979919433593]\n",
      "\t a: [14, 599, 0.7960417938232421]\n",
      "\t l: [14, 799, 1.3219244384765625]\n",
      "\t a: [14, 799, 0.7958334350585937]\n",
      "\t l: [14, 999, 1.2501720428466796]\n",
      "\t a: [14, 999, 0.8154167175292969]\n",
      "\t l: [14, 1199, 1.273372039794922]\n",
      "\t a: [14, 1199, 0.8062500762939453]\n",
      "\t l: [14, 1399, 1.3798146057128906]\n",
      "\t a: [14, 1399, 0.7791667938232422]\n",
      "\t l: [14, 1599, 1.2580844116210939]\n",
      "\t a: [14, 1599, 0.8183334350585938]\n",
      "\t l: [14, 1799, 1.3237843322753906]\n",
      "\t a: [14, 1799, 0.7972913360595704]\n",
      "\t l: [14, 1999, 1.2998374938964843]\n",
      "\t a: [14, 1999, 0.8056249237060547]\n",
      "\t l: [14, 2199, 1.265259017944336]\n",
      "\t a: [14, 2199, 0.8075000762939453]\n",
      "\t l: [14, 2399, 1.221483154296875]\n",
      "\t a: [14, 2399, 0.8210416412353516]\n",
      "\t Validation\n",
      "\t l: [14, 416, 2.5797344970703127]\n",
      "\t a: [14, 416, 1.7082292175292968]\n",
      "--- Epoch : 15 ---\n",
      "\t Training\n",
      "\t l: [15, 199, 1.2363871765136718]\n",
      "\t a: [15, 199, 0.8143751525878906]\n",
      "\t l: [15, 399, 1.274368896484375]\n",
      "\t a: [15, 399, 0.8083335113525391]\n",
      "\t l: [15, 599, 1.2825259399414062]\n",
      "\t a: [15, 599, 0.7999996948242187]\n",
      "\t l: [15, 799, 1.303279266357422]\n",
      "\t a: [15, 799, 0.7991669464111328]\n",
      "\t l: [15, 999, 1.2330450439453124]\n",
      "\t a: [15, 999, 0.8160415649414062]\n",
      "\t l: [15, 1199, 1.259353790283203]\n",
      "\t a: [15, 1199, 0.80625]\n",
      "\t l: [15, 1399, 1.363202362060547]\n",
      "\t a: [15, 1399, 0.7812496948242188]\n",
      "\t l: [15, 1599, 1.241408920288086]\n",
      "\t a: [15, 1599, 0.8183332061767579]\n",
      "\t l: [15, 1799, 1.3135675048828126]\n",
      "\t a: [15, 1799, 0.8002082824707031]\n",
      "\t l: [15, 1999, 1.287130126953125]\n",
      "\t a: [15, 1999, 0.8033332824707031]\n",
      "\t l: [15, 2199, 1.2530350494384765]\n",
      "\t a: [15, 2199, 0.8052083587646485]\n",
      "\t l: [15, 2399, 1.2078369903564452]\n",
      "\t a: [15, 2399, 0.8239585113525391]\n",
      "\t Validation\n",
      "\t l: [15, 416, 2.5464573669433594]\n",
      "\t a: [15, 416, 1.7088548278808593]\n",
      "--- Epoch : 16 ---\n",
      "\t Training\n",
      "\t l: [16, 199, 1.2226990509033202]\n",
      "\t a: [16, 199, 0.8160415649414062]\n",
      "\t l: [16, 399, 1.2666558837890625]\n",
      "\t a: [16, 399, 0.8095835876464844]\n",
      "\t l: [16, 599, 1.2696226501464845]\n",
      "\t a: [16, 599, 0.8043746948242188]\n",
      "\t l: [16, 799, 1.2889466857910157]\n",
      "\t a: [16, 799, 0.799375]\n",
      "\t l: [16, 999, 1.215712890625]\n",
      "\t a: [16, 999, 0.8191665649414063]\n",
      "\t l: [16, 1199, 1.2437476348876952]\n",
      "\t a: [16, 1199, 0.8106251525878906]\n",
      "\t l: [16, 1399, 1.3506631469726562]\n",
      "\t a: [16, 1399, 0.7804168701171875]\n",
      "\t l: [16, 1599, 1.22501953125]\n",
      "\t a: [16, 1599, 0.8185415649414063]\n",
      "\t l: [16, 1799, 1.3052857971191407]\n",
      "\t a: [16, 1799, 0.7991665649414063]\n",
      "\t l: [16, 1999, 1.273154754638672]\n",
      "\t a: [16, 1999, 0.805625]\n",
      "\t l: [16, 2199, 1.2420674133300782]\n",
      "\t a: [16, 2199, 0.805]\n",
      "\t l: [16, 2399, 1.1926177978515624]\n",
      "\t a: [16, 2399, 0.8266667175292969]\n",
      "\t Validation\n",
      "\t l: [16, 416, 2.5163560485839844]\n",
      "\t a: [16, 416, 1.7146881103515625]\n",
      "--- Epoch : 17 ---\n",
      "\t Training\n",
      "\t l: [17, 199, 1.2088818359375]\n",
      "\t a: [17, 199, 0.8189581298828125]\n",
      "\t l: [17, 399, 1.2547474670410157]\n",
      "\t a: [17, 399, 0.8102082824707031]\n",
      "\t l: [17, 599, 1.2589200592041017]\n",
      "\t a: [17, 599, 0.8033332824707031]\n",
      "\t l: [17, 799, 1.2728994750976563]\n",
      "\t a: [17, 799, 0.7995829772949219]\n",
      "\t l: [17, 999, 1.2040595245361327]\n",
      "\t a: [17, 999, 0.8212496185302735]\n",
      "\t l: [17, 1199, 1.232935562133789]\n",
      "\t a: [17, 1199, 0.8091668701171875]\n",
      "\t l: [17, 1399, 1.3435372924804687]\n",
      "\t a: [17, 1399, 0.7839583587646485]\n",
      "\t l: [17, 1599, 1.2109944915771484]\n",
      "\t a: [17, 1599, 0.8179167175292968]\n",
      "\t l: [17, 1799, 1.2866909790039063]\n",
      "\t a: [17, 1799, 0.8016664886474609]\n",
      "\t l: [17, 1999, 1.2568157196044922]\n",
      "\t a: [17, 1999, 0.8079167175292968]\n",
      "\t l: [17, 2199, 1.2290376281738282]\n",
      "\t a: [17, 2199, 0.8077082824707031]\n",
      "\t l: [17, 2399, 1.172184066772461]\n",
      "\t a: [17, 2399, 0.8260415649414062]\n",
      "\t Validation\n",
      "\t l: [17, 416, 2.4772830200195313]\n",
      "\t a: [17, 416, 1.717187957763672]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 4.462510070800781]\n",
      "\t a: [0, 199, 0.11958335876464844]\n",
      "\t l: [0, 399, 3.4535696411132815]\n",
      "\t a: [0, 399, 0.14520833969116212]\n",
      "\t l: [0, 599, 3.2978530883789063]\n",
      "\t a: [0, 599, 0.15937500953674316]\n",
      "\t l: [0, 799, 3.1967709350585936]\n",
      "\t a: [0, 799, 0.1947915458679199]\n",
      "\t l: [0, 999, 3.1342404174804686]\n",
      "\t a: [0, 999, 0.22958332061767578]\n",
      "\t l: [0, 1199, 3.0474359130859376]\n",
      "\t a: [0, 1199, 0.28208341598510744]\n",
      "\t l: [0, 1399, 3.006785583496094]\n",
      "\t a: [0, 1399, 0.28729169845581054]\n",
      "\t l: [0, 1599, 2.9454071044921877]\n",
      "\t a: [0, 1599, 0.32312496185302736]\n",
      "\t l: [0, 1799, 2.8805377197265627]\n",
      "\t a: [0, 1799, 0.35125003814697264]\n",
      "\t l: [0, 1999, 2.80519775390625]\n",
      "\t a: [0, 1999, 0.38812503814697263]\n",
      "\t l: [0, 2199, 2.7552896118164063]\n",
      "\t a: [0, 2199, 0.409999885559082]\n",
      "\t l: [0, 2399, 2.691769714355469]\n",
      "\t a: [0, 2399, 0.44208335876464844]\n",
      "\t Validation\n",
      "\t l: [0, 416, 5.491210327148438]\n",
      "\t a: [0, 416, 0.9686458587646485]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.6270266723632814]\n",
      "\t a: [1, 199, 0.4702083969116211]\n",
      "\t l: [1, 399, 2.5846023559570312]\n",
      "\t a: [1, 399, 0.4916668701171875]\n",
      "\t l: [1, 599, 2.5404656982421874]\n",
      "\t a: [1, 599, 0.5072915649414063]\n",
      "\t l: [1, 799, 2.530626068115234]\n",
      "\t a: [1, 799, 0.5141665267944336]\n",
      "\t l: [1, 999, 2.46087646484375]\n",
      "\t a: [1, 999, 0.5356248092651367]\n",
      "\t l: [1, 1199, 2.421519012451172]\n",
      "\t a: [1, 1199, 0.553749885559082]\n",
      "\t l: [1, 1399, 2.4576727294921876]\n",
      "\t a: [1, 1399, 0.5393749237060547]\n",
      "\t l: [1, 1599, 2.381248016357422]\n",
      "\t a: [1, 1599, 0.5702083587646485]\n",
      "\t l: [1, 1799, 2.351998138427734]\n",
      "\t a: [1, 1799, 0.5893750762939454]\n",
      "\t l: [1, 1999, 2.30419921875]\n",
      "\t a: [1, 1999, 0.6037501525878907]\n",
      "\t l: [1, 2199, 2.2741871643066407]\n",
      "\t a: [1, 2199, 0.6100000762939453]\n",
      "\t l: [1, 2399, 2.216561584472656]\n",
      "\t a: [1, 2399, 0.6287498474121094]\n",
      "\t Validation\n",
      "\t l: [1, 416, 4.552571716308594]\n",
      "\t a: [1, 416, 1.322812042236328]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.183108062744141]\n",
      "\t a: [2, 199, 0.6337500381469726]\n",
      "\t l: [2, 399, 2.1645130920410156]\n",
      "\t a: [2, 399, 0.6377082443237305]\n",
      "\t l: [2, 599, 2.1368247985839846]\n",
      "\t a: [2, 599, 0.6414585876464843]\n",
      "\t l: [2, 799, 2.1515248107910154]\n",
      "\t a: [2, 799, 0.6395830917358398]\n",
      "\t l: [2, 999, 2.080089874267578]\n",
      "\t a: [2, 999, 0.661875]\n",
      "\t l: [2, 1199, 2.055899963378906]\n",
      "\t a: [2, 1199, 0.6668749237060547]\n",
      "\t l: [2, 1399, 2.1178338623046873]\n",
      "\t a: [2, 1399, 0.6443749237060546]\n",
      "\t l: [2, 1599, 2.038718566894531]\n",
      "\t a: [2, 1599, 0.675625]\n",
      "\t l: [2, 1799, 2.0363987731933593]\n",
      "\t a: [2, 1799, 0.6766669464111328]\n",
      "\t l: [2, 1999, 1.996625213623047]\n",
      "\t a: [2, 1999, 0.6977082824707032]\n",
      "\t l: [2, 2199, 1.9784733581542968]\n",
      "\t a: [2, 2199, 0.6958332824707031]\n",
      "\t l: [2, 2399, 1.927811737060547]\n",
      "\t a: [2, 2399, 0.7077082061767578]\n",
      "\t Validation\n",
      "\t l: [2, 416, 3.9805938720703127]\n",
      "\t a: [2, 416, 1.4818743896484374]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 1.910299072265625]\n",
      "\t a: [3, 199, 0.7124998474121094]\n",
      "\t l: [3, 399, 1.909228057861328]\n",
      "\t a: [3, 399, 0.7047917938232422]\n",
      "\t l: [3, 599, 1.8969606018066407]\n",
      "\t a: [3, 599, 0.7033334350585938]\n",
      "\t l: [3, 799, 1.9144490051269532]\n",
      "\t a: [3, 799, 0.7068749237060546]\n",
      "\t l: [3, 999, 1.8486015319824218]\n",
      "\t a: [3, 999, 0.7250001525878906]\n",
      "\t l: [3, 1199, 1.8357450866699219]\n",
      "\t a: [3, 1199, 0.7247917175292968]\n",
      "\t l: [3, 1399, 1.9161595153808593]\n",
      "\t a: [3, 1399, 0.69875]\n",
      "\t l: [3, 1599, 1.8275372314453124]\n",
      "\t a: [3, 1599, 0.7277080535888671]\n",
      "\t l: [3, 1799, 1.8364547729492187]\n",
      "\t a: [3, 1799, 0.7235416412353516]\n",
      "\t l: [3, 1999, 1.7996337890625]\n",
      "\t a: [3, 1999, 0.7447916412353516]\n",
      "\t l: [3, 2199, 1.7893048095703126]\n",
      "\t a: [3, 2199, 0.7391664123535157]\n",
      "\t l: [3, 2399, 1.740474853515625]\n",
      "\t a: [3, 2399, 0.756875]\n",
      "\t Validation\n",
      "\t l: [3, 416, 3.6083819580078127]\n",
      "\t a: [3, 416, 1.5797921752929687]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 1.7335275268554688]\n",
      "\t a: [4, 199, 0.7554167938232422]\n",
      "\t l: [4, 399, 1.7404994201660156]\n",
      "\t a: [4, 399, 0.75]\n",
      "\t l: [4, 599, 1.7310183715820313]\n",
      "\t a: [4, 599, 0.7410417175292969]\n",
      "\t l: [4, 799, 1.7577859497070312]\n",
      "\t a: [4, 799, 0.7489583587646484]\n",
      "\t l: [4, 999, 1.698926239013672]\n",
      "\t a: [4, 999, 0.7600000762939453]\n",
      "\t l: [4, 1199, 1.6919491577148438]\n",
      "\t a: [4, 1199, 0.7610417175292968]\n",
      "\t l: [4, 1399, 1.7774639892578126]\n",
      "\t a: [4, 1399, 0.731666488647461]\n",
      "\t l: [4, 1599, 1.6864686584472657]\n",
      "\t a: [4, 1599, 0.7610414123535156]\n",
      "\t l: [4, 1799, 1.7010842895507812]\n",
      "\t a: [4, 1799, 0.7535419464111328]\n",
      "\t l: [4, 1999, 1.6649093627929688]\n",
      "\t a: [4, 1999, 0.7712499237060547]\n",
      "\t l: [4, 2199, 1.657588348388672]\n",
      "\t a: [4, 2199, 0.7635414123535156]\n",
      "\t l: [4, 2399, 1.6088685607910156]\n",
      "\t a: [4, 2399, 0.7850001525878906]\n",
      "\t Validation\n",
      "\t l: [4, 416, 3.3434417724609373]\n",
      "\t a: [4, 416, 1.6294798278808593]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 1.6094363403320313]\n",
      "\t a: [5, 199, 0.775]\n",
      "\t l: [5, 399, 1.6201094055175782]\n",
      "\t a: [5, 399, 0.7729167938232422]\n",
      "\t l: [5, 599, 1.6161380004882813]\n",
      "\t a: [5, 599, 0.7618753051757813]\n",
      "\t l: [5, 799, 1.643794708251953]\n",
      "\t a: [5, 799, 0.7672917175292969]\n",
      "\t l: [5, 999, 1.5834251403808595]\n",
      "\t a: [5, 999, 0.7810417938232422]\n",
      "\t l: [5, 1199, 1.582575225830078]\n",
      "\t a: [5, 1199, 0.7818751525878906]\n",
      "\t l: [5, 1399, 1.673356475830078]\n",
      "\t a: [5, 1399, 0.7495831298828125]\n",
      "\t l: [5, 1599, 1.5809605407714844]\n",
      "\t a: [5, 1599, 0.7845833587646485]\n",
      "\t l: [5, 1799, 1.6011030578613281]\n",
      "\t a: [5, 1799, 0.7700000762939453]\n",
      "\t l: [5, 1999, 1.5680311584472657]\n",
      "\t a: [5, 1999, 0.7874999237060547]\n",
      "\t l: [5, 2199, 1.5620947265625]\n",
      "\t a: [5, 2199, 0.7822917175292968]\n",
      "\t l: [5, 2399, 1.511216278076172]\n",
      "\t a: [5, 2399, 0.806875]\n",
      "\t Validation\n",
      "\t l: [5, 416, 3.1399496459960936]\n",
      "\t a: [5, 416, 1.6661460876464844]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 1.5139657592773437]\n",
      "\t a: [6, 199, 0.7893750762939453]\n",
      "\t l: [6, 399, 1.528894805908203]\n",
      "\t a: [6, 399, 0.7906250762939453]\n",
      "\t l: [6, 599, 1.5264944458007812]\n",
      "\t a: [6, 599, 0.7818751525878906]\n",
      "\t l: [6, 799, 1.5558467102050781]\n",
      "\t a: [6, 799, 0.7845831298828125]\n",
      "\t l: [6, 999, 1.498377685546875]\n",
      "\t a: [6, 999, 0.7918749237060547]\n",
      "\t l: [6, 1199, 1.4982376098632812]\n",
      "\t a: [6, 1199, 0.7956246948242187]\n",
      "\t l: [6, 1399, 1.5908087158203126]\n",
      "\t a: [6, 1399, 0.7689582824707031]\n",
      "\t l: [6, 1599, 1.4958262634277344]\n",
      "\t a: [6, 1599, 0.8010414886474609]\n",
      "\t l: [6, 1799, 1.5204466247558595]\n",
      "\t a: [6, 1799, 0.7895833587646485]\n",
      "\t l: [6, 1999, 1.490270538330078]\n",
      "\t a: [6, 1999, 0.8024999237060547]\n",
      "\t l: [6, 2199, 1.4822457885742188]\n",
      "\t a: [6, 2199, 0.7981246185302734]\n",
      "\t l: [6, 2399, 1.4354452514648437]\n",
      "\t a: [6, 2399, 0.8233332061767578]\n",
      "\t Validation\n",
      "\t l: [6, 416, 2.987334899902344]\n",
      "\t a: [6, 416, 1.6947929382324218]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.4412477111816406]\n",
      "\t a: [7, 199, 0.8054166412353516]\n",
      "\t l: [7, 399, 1.4575962829589844]\n",
      "\t a: [7, 399, 0.8064583587646484]\n",
      "\t l: [7, 599, 1.459193572998047]\n",
      "\t a: [7, 599, 0.7950000762939453]\n",
      "\t l: [7, 799, 1.4845013427734375]\n",
      "\t a: [7, 799, 0.7958331298828125]\n",
      "\t l: [7, 999, 1.426036376953125]\n",
      "\t a: [7, 999, 0.8083329010009765]\n",
      "\t l: [7, 1199, 1.4272755432128905]\n",
      "\t a: [7, 1199, 0.8116666412353516]\n",
      "\t l: [7, 1399, 1.5241279602050781]\n",
      "\t a: [7, 1399, 0.7827082824707031]\n",
      "\t l: [7, 1599, 1.4267768859863281]\n",
      "\t a: [7, 1599, 0.8114582824707032]\n",
      "\t l: [7, 1799, 1.4563589477539063]\n",
      "\t a: [7, 1799, 0.7993751525878906]\n",
      "\t l: [7, 1999, 1.4260623168945312]\n",
      "\t a: [7, 1999, 0.81]\n",
      "\t l: [7, 2199, 1.4216580200195312]\n",
      "\t a: [7, 2199, 0.8049998474121094]\n",
      "\t l: [7, 2399, 1.3734001159667968]\n",
      "\t a: [7, 2399, 0.8299999237060547]\n",
      "\t Validation\n",
      "\t l: [7, 416, 2.85712158203125]\n",
      "\t a: [7, 416, 1.7173963928222655]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.378455810546875]\n",
      "\t a: [8, 199, 0.8164582061767578]\n",
      "\t l: [8, 399, 1.395953369140625]\n",
      "\t a: [8, 399, 0.8150001525878906]\n",
      "\t l: [8, 599, 1.400183868408203]\n",
      "\t a: [8, 599, 0.805]\n",
      "\t l: [8, 799, 1.4264437866210937]\n",
      "\t a: [8, 799, 0.8054166412353516]\n",
      "\t l: [8, 999, 1.3690278625488281]\n",
      "\t a: [8, 999, 0.815416488647461]\n",
      "\t l: [8, 1199, 1.3719317626953125]\n",
      "\t a: [8, 1199, 0.821875]\n",
      "\t l: [8, 1399, 1.4712208557128905]\n",
      "\t a: [8, 1399, 0.7885414123535156]\n",
      "\t l: [8, 1599, 1.3712939453125]\n",
      "\t a: [8, 1599, 0.8191667938232422]\n",
      "\t l: [8, 1799, 1.4029185485839843]\n",
      "\t a: [8, 1799, 0.8060417175292969]\n",
      "\t l: [8, 1999, 1.3721299743652344]\n",
      "\t a: [8, 1999, 0.8172917938232422]\n",
      "\t l: [8, 2199, 1.3669664001464843]\n",
      "\t a: [8, 2199, 0.8152082061767578]\n",
      "\t l: [8, 2399, 1.31861328125]\n",
      "\t a: [8, 2399, 0.8397914886474609]\n",
      "\t Validation\n",
      "\t l: [8, 416, 2.743126220703125]\n",
      "\t a: [8, 416, 1.7336463928222656]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.3238018798828124]\n",
      "\t a: [9, 199, 0.8227080535888672]\n",
      "\t l: [9, 399, 1.3425408935546874]\n",
      "\t a: [9, 399, 0.8245833587646484]\n",
      "\t l: [9, 599, 1.3511659240722655]\n",
      "\t a: [9, 599, 0.813958511352539]\n",
      "\t l: [9, 799, 1.3740330505371094]\n",
      "\t a: [9, 799, 0.8166664886474609]\n",
      "\t l: [9, 999, 1.3207518005371093]\n",
      "\t a: [9, 999, 0.8249999237060547]\n",
      "\t l: [9, 1199, 1.3250352478027343]\n",
      "\t a: [9, 1199, 0.8256248474121094]\n",
      "\t l: [9, 1399, 1.425572052001953]\n",
      "\t a: [9, 1399, 0.7958334350585937]\n",
      "\t l: [9, 1599, 1.323321075439453]\n",
      "\t a: [9, 1599, 0.8272916412353516]\n",
      "\t l: [9, 1799, 1.3597512817382813]\n",
      "\t a: [9, 1799, 0.8122916412353516]\n",
      "\t l: [9, 1999, 1.3268449401855469]\n",
      "\t a: [9, 1999, 0.8229167175292968]\n",
      "\t l: [9, 2199, 1.3245455932617187]\n",
      "\t a: [9, 2199, 0.8220835113525391]\n",
      "\t l: [9, 2399, 1.2769600677490234]\n",
      "\t a: [9, 2399, 0.842500228881836]\n",
      "\t Validation\n",
      "\t l: [9, 416, 2.659223937988281]\n",
      "\t a: [9, 416, 1.7440628051757812]\n",
      "--- Epoch : 10 ---\n",
      "\t Training\n",
      "\t l: [10, 199, 1.2847067260742187]\n",
      "\t a: [10, 199, 0.8322917175292969]\n",
      "\t l: [10, 399, 1.305198211669922]\n",
      "\t a: [10, 399, 0.8277084350585937]\n",
      "\t l: [10, 599, 1.315526885986328]\n",
      "\t a: [10, 599, 0.8164584350585937]\n",
      "\t l: [10, 799, 1.3345652770996095]\n",
      "\t a: [10, 799, 0.8239582824707031]\n",
      "\t l: [10, 999, 1.281905975341797]\n",
      "\t a: [10, 999, 0.8312498474121094]\n",
      "\t l: [10, 1199, 1.2853167724609376]\n",
      "\t a: [10, 1199, 0.8320835113525391]\n",
      "\t l: [10, 1399, 1.3885896301269531]\n",
      "\t a: [10, 1399, 0.8008333587646485]\n",
      "\t l: [10, 1599, 1.2875621032714843]\n",
      "\t a: [10, 1599, 0.8291667175292968]\n",
      "\t l: [10, 1799, 1.3247187805175782]\n",
      "\t a: [10, 1799, 0.8189582824707031]\n",
      "\t l: [10, 1999, 1.2938674926757812]\n",
      "\t a: [10, 1999, 0.8283332061767578]\n",
      "\t l: [10, 2199, 1.292741241455078]\n",
      "\t a: [10, 2199, 0.8266667938232422]\n",
      "\t l: [10, 2399, 1.2419216918945313]\n",
      "\t a: [10, 2399, 0.8460418701171875]\n",
      "\t Validation\n",
      "\t l: [10, 416, 2.5891183471679686]\n",
      "\t a: [10, 416, 1.7502081298828125]\n",
      "--- Epoch : 11 ---\n",
      "\t Training\n",
      "\t l: [11, 199, 1.2483306121826172]\n",
      "\t a: [11, 199, 0.836041259765625]\n",
      "\t l: [11, 399, 1.2708562469482423]\n",
      "\t a: [11, 399, 0.8308334350585938]\n",
      "\t l: [11, 599, 1.2808389282226562]\n",
      "\t a: [11, 599, 0.8216665649414062]\n",
      "\t l: [11, 799, 1.30183349609375]\n",
      "\t a: [11, 799, 0.8256249237060547]\n",
      "\t l: [11, 999, 1.2494007110595704]\n",
      "\t a: [11, 999, 0.8341664886474609]\n",
      "\t l: [11, 1199, 1.2534378051757813]\n",
      "\t a: [11, 1199, 0.8360419464111328]\n",
      "\t l: [11, 1399, 1.35558837890625]\n",
      "\t a: [11, 1399, 0.8064582061767578]\n",
      "\t l: [11, 1599, 1.2531878662109375]\n",
      "\t a: [11, 1599, 0.8368751525878906]\n",
      "\t l: [11, 1799, 1.2918759155273438]\n",
      "\t a: [11, 1799, 0.8235414886474609]\n",
      "\t l: [11, 1999, 1.2582955169677734]\n",
      "\t a: [11, 1999, 0.8318747711181641]\n",
      "\t l: [11, 2199, 1.2595604705810546]\n",
      "\t a: [11, 2199, 0.8302084350585938]\n",
      "\t l: [11, 2399, 1.2076302337646485]\n",
      "\t a: [11, 2399, 0.851250228881836]\n",
      "\t Validation\n",
      "\t l: [11, 416, 2.522095489501953]\n",
      "\t a: [11, 416, 1.7615625]\n",
      "--- Epoch : 12 ---\n",
      "\t Training\n",
      "\t l: [12, 199, 1.2157067108154296]\n",
      "\t a: [12, 199, 0.8424998474121094]\n",
      "\t l: [12, 399, 1.237837448120117]\n",
      "\t a: [12, 399, 0.835625]\n",
      "\t l: [12, 599, 1.2514407348632812]\n",
      "\t a: [12, 599, 0.825416488647461]\n",
      "\t l: [12, 799, 1.2726966094970704]\n",
      "\t a: [12, 799, 0.8302080535888672]\n",
      "\t l: [12, 999, 1.2205311584472656]\n",
      "\t a: [12, 999, 0.8379164123535157]\n",
      "\t l: [12, 1199, 1.225090789794922]\n",
      "\t a: [12, 1199, 0.8402085876464844]\n",
      "\t l: [12, 1399, 1.3297621154785155]\n",
      "\t a: [12, 1399, 0.8095831298828124]\n",
      "\t l: [12, 1599, 1.2236315155029296]\n",
      "\t a: [12, 1599, 0.8418749237060547]\n",
      "\t l: [12, 1799, 1.2629629516601562]\n",
      "\t a: [12, 1799, 0.8272916412353516]\n",
      "\t l: [12, 1999, 1.230567626953125]\n",
      "\t a: [12, 1999, 0.8364583587646485]\n",
      "\t l: [12, 2199, 1.2314707946777343]\n",
      "\t a: [12, 2199, 0.832500228881836]\n",
      "\t l: [12, 2399, 1.1806912994384766]\n",
      "\t a: [12, 2399, 0.8539585113525391]\n",
      "\t Validation\n",
      "\t l: [12, 416, 2.4637953186035157]\n",
      "\t a: [12, 416, 1.7665623474121093]\n",
      "--- Epoch : 13 ---\n",
      "\t Training\n",
      "\t l: [13, 199, 1.1864788818359375]\n",
      "\t a: [13, 199, 0.844999771118164]\n",
      "\t l: [13, 399, 1.2087919616699219]\n",
      "\t a: [13, 399, 0.8397917175292968]\n",
      "\t l: [13, 599, 1.2217405700683595]\n",
      "\t a: [13, 599, 0.8297915649414063]\n",
      "\t l: [13, 799, 1.2456878662109374]\n",
      "\t a: [13, 799, 0.8331246185302734]\n",
      "\t l: [13, 999, 1.1927875518798827]\n",
      "\t a: [13, 999, 0.8397911834716797]\n",
      "\t l: [13, 1199, 1.198920364379883]\n",
      "\t a: [13, 1199, 0.8412501525878906]\n",
      "\t l: [13, 1399, 1.3048019409179688]\n",
      "\t a: [13, 1399, 0.8139579772949219]\n",
      "\t l: [13, 1599, 1.1968216705322265]\n",
      "\t a: [13, 1599, 0.8460417938232422]\n",
      "\t l: [13, 1799, 1.2369342041015625]\n",
      "\t a: [13, 1799, 0.8308332824707031]\n",
      "\t l: [13, 1999, 1.205643310546875]\n",
      "\t a: [13, 1999, 0.8385415649414063]\n",
      "\t l: [13, 2199, 1.207169952392578]\n",
      "\t a: [13, 2199, 0.8370835113525391]\n",
      "\t l: [13, 2399, 1.153895492553711]\n",
      "\t a: [13, 2399, 0.8575001525878906]\n",
      "\t Validation\n",
      "\t l: [13, 416, 2.4119866943359374]\n",
      "\t a: [13, 416, 1.7784378051757812]\n",
      "--- Epoch : 14 ---\n",
      "\t Training\n",
      "\t l: [14, 199, 1.162722396850586]\n",
      "\t a: [14, 199, 0.8462496185302735]\n",
      "\t l: [14, 399, 1.1862102508544923]\n",
      "\t a: [14, 399, 0.8439583587646484]\n",
      "\t l: [14, 599, 1.2004495239257813]\n",
      "\t a: [14, 599, 0.83375]\n",
      "\t l: [14, 799, 1.2205828094482423]\n",
      "\t a: [14, 799, 0.8362496185302735]\n",
      "\t l: [14, 999, 1.1696929168701171]\n",
      "\t a: [14, 999, 0.8443747711181641]\n",
      "\t l: [14, 1199, 1.1758412170410155]\n",
      "\t a: [14, 1199, 0.8479166412353516]\n",
      "\t l: [14, 1399, 1.2794477081298827]\n",
      "\t a: [14, 1399, 0.8193748474121094]\n",
      "\t l: [14, 1599, 1.1727159881591798]\n",
      "\t a: [14, 1599, 0.8502084350585938]\n",
      "\t l: [14, 1799, 1.2170234680175782]\n",
      "\t a: [14, 1799, 0.8339582061767579]\n",
      "\t l: [14, 1999, 1.180898208618164]\n",
      "\t a: [14, 1999, 0.8414581298828125]\n",
      "\t l: [14, 2199, 1.1830686187744142]\n",
      "\t a: [14, 2199, 0.8410417175292969]\n",
      "\t l: [14, 2399, 1.1303619384765624]\n",
      "\t a: [14, 2399, 0.8625000762939453]\n",
      "\t Validation\n",
      "\t l: [14, 416, 2.363782501220703]\n",
      "\t a: [14, 416, 1.7834381103515624]\n",
      "--- Epoch : 15 ---\n",
      "\t Training\n",
      "\t l: [15, 199, 1.139128189086914]\n",
      "\t a: [15, 199, 0.8539579772949218]\n",
      "\t l: [15, 399, 1.1630368041992187]\n",
      "\t a: [15, 399, 0.8449998474121094]\n",
      "\t l: [15, 599, 1.177142333984375]\n",
      "\t a: [15, 599, 0.8366667175292969]\n",
      "\t l: [15, 799, 1.1944310760498047]\n",
      "\t a: [15, 799, 0.8399996185302734]\n",
      "\t l: [15, 999, 1.1449701690673828]\n",
      "\t a: [15, 999, 0.8477080535888671]\n",
      "\t l: [15, 1199, 1.1505889892578125]\n",
      "\t a: [15, 1199, 0.8512500762939453]\n",
      "\t l: [15, 1399, 1.2550043487548828]\n",
      "\t a: [15, 1399, 0.81875]\n",
      "\t l: [15, 1599, 1.149007797241211]\n",
      "\t a: [15, 1599, 0.8510417175292969]\n",
      "\t l: [15, 1799, 1.1924140167236328]\n",
      "\t a: [15, 1799, 0.8360414123535156]\n",
      "\t l: [15, 1999, 1.1589608764648438]\n",
      "\t a: [15, 1999, 0.8433329772949218]\n",
      "\t l: [15, 2199, 1.1608976745605468]\n",
      "\t a: [15, 2199, 0.844375]\n",
      "\t l: [15, 2399, 1.108531265258789]\n",
      "\t a: [15, 2399, 0.8625]\n",
      "\t Validation\n",
      "\t l: [15, 416, 2.318442535400391]\n",
      "\t a: [15, 416, 1.786458740234375]\n",
      "--- Epoch : 16 ---\n",
      "\t Training\n",
      "\t l: [16, 199, 1.117513427734375]\n",
      "\t a: [16, 199, 0.8554164123535156]\n",
      "\t l: [16, 399, 1.142402114868164]\n",
      "\t a: [16, 399, 0.8474999237060546]\n",
      "\t l: [16, 599, 1.1582992553710938]\n",
      "\t a: [16, 599, 0.8377082061767578]\n",
      "\t l: [16, 799, 1.1752920532226563]\n",
      "\t a: [16, 799, 0.8445830535888672]\n",
      "\t l: [16, 999, 1.1239601135253907]\n",
      "\t a: [16, 999, 0.8512499237060547]\n",
      "\t l: [16, 1199, 1.1317051696777343]\n",
      "\t a: [16, 1199, 0.8541665649414063]\n",
      "\t l: [16, 1399, 1.2380421447753907]\n",
      "\t a: [16, 1399, 0.8235417938232422]\n",
      "\t l: [16, 1599, 1.1289833068847657]\n",
      "\t a: [16, 1599, 0.856458511352539]\n",
      "\t l: [16, 1799, 1.1756416320800782]\n",
      "\t a: [16, 1799, 0.839999771118164]\n",
      "\t l: [16, 1999, 1.1417626190185546]\n",
      "\t a: [16, 1999, 0.8452081298828125]\n",
      "\t l: [16, 2199, 1.1433963775634766]\n",
      "\t a: [16, 2199, 0.84625]\n",
      "\t l: [16, 2399, 1.0907421875]\n",
      "\t a: [16, 2399, 0.8689582824707032]\n",
      "\t Validation\n",
      "\t l: [16, 416, 2.279676818847656]\n",
      "\t a: [16, 416, 1.790521240234375]\n",
      "--- Epoch : 17 ---\n",
      "\t Training\n",
      "\t l: [17, 199, 1.099177474975586]\n",
      "\t a: [17, 199, 0.8552081298828125]\n",
      "\t l: [17, 399, 1.12332275390625]\n",
      "\t a: [17, 399, 0.851875]\n",
      "\t l: [17, 599, 1.141859130859375]\n",
      "\t a: [17, 599, 0.8402082824707031]\n",
      "\t l: [17, 799, 1.1554019927978516]\n",
      "\t a: [17, 799, 0.8454164123535156]\n",
      "\t l: [17, 999, 1.1062319946289063]\n",
      "\t a: [17, 999, 0.8529163360595703]\n",
      "\t l: [17, 1199, 1.1121072387695312]\n",
      "\t a: [17, 1199, 0.8568749237060547]\n",
      "\t l: [17, 1399, 1.2182984161376953]\n",
      "\t a: [17, 1399, 0.8256248474121094]\n",
      "\t l: [17, 1599, 1.1091741180419923]\n",
      "\t a: [17, 1599, 0.8568751525878906]\n",
      "\t l: [17, 1799, 1.155774154663086]\n",
      "\t a: [17, 1799, 0.8418747711181641]\n",
      "\t l: [17, 1999, 1.1224043273925781]\n",
      "\t a: [17, 1999, 0.8477082061767578]\n",
      "\t l: [17, 2199, 1.12598388671875]\n",
      "\t a: [17, 2199, 0.8464582061767578]\n",
      "\t l: [17, 2399, 1.0735283660888673]\n",
      "\t a: [17, 2399, 0.8675001525878906]\n",
      "\t Validation\n",
      "\t l: [17, 416, 2.2433021545410154]\n",
      "\t a: [17, 416, 1.7928126525878907]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 4.91766845703125]\n",
      "\t a: [0, 199, 0.08145832061767579]\n",
      "\t l: [0, 399, 3.488431701660156]\n",
      "\t a: [0, 399, 0.09604166030883789]\n",
      "\t l: [0, 599, 3.332030029296875]\n",
      "\t a: [0, 599, 0.12312500953674316]\n",
      "\t l: [0, 799, 3.2321102905273436]\n",
      "\t a: [0, 799, 0.1602083396911621]\n",
      "\t l: [0, 999, 3.1500360107421876]\n",
      "\t a: [0, 999, 0.19937498092651368]\n",
      "\t l: [0, 1199, 3.082761535644531]\n",
      "\t a: [0, 1199, 0.23229158401489258]\n",
      "\t l: [0, 1399, 3.0347189331054687]\n",
      "\t a: [0, 1399, 0.25604164123535156]\n",
      "\t l: [0, 1599, 2.946824951171875]\n",
      "\t a: [0, 1599, 0.29708324432373046]\n",
      "\t l: [0, 1799, 2.862093505859375]\n",
      "\t a: [0, 1799, 0.35416656494140625]\n",
      "\t l: [0, 1999, 2.833511962890625]\n",
      "\t a: [0, 1999, 0.36916656494140626]\n",
      "\t l: [0, 2199, 2.7646893310546874]\n",
      "\t a: [0, 2199, 0.4050000762939453]\n",
      "\t l: [0, 2399, 2.6869732666015627]\n",
      "\t a: [0, 2399, 0.4627082824707031]\n",
      "\t Validation\n",
      "\t l: [0, 416, 5.482118530273437]\n",
      "\t a: [0, 416, 1.002291717529297]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.609668884277344]\n",
      "\t a: [1, 199, 0.49104167938232424]\n",
      "\t l: [1, 399, 2.566119384765625]\n",
      "\t a: [1, 399, 0.5035417175292969]\n",
      "\t l: [1, 599, 2.5232554626464845]\n",
      "\t a: [1, 599, 0.5310417556762695]\n",
      "\t l: [1, 799, 2.5097201538085936]\n",
      "\t a: [1, 799, 0.5225]\n",
      "\t l: [1, 999, 2.4227645874023436]\n",
      "\t a: [1, 999, 0.5702082443237305]\n",
      "\t l: [1, 1199, 2.39906982421875]\n",
      "\t a: [1, 1199, 0.5631249237060547]\n",
      "\t l: [1, 1399, 2.4255519104003906]\n",
      "\t a: [1, 1399, 0.5629168319702148]\n",
      "\t l: [1, 1599, 2.334347991943359]\n",
      "\t a: [1, 1599, 0.5987500381469727]\n",
      "\t l: [1, 1799, 2.2985549926757813]\n",
      "\t a: [1, 1799, 0.6029167556762696]\n",
      "\t l: [1, 1999, 2.286579437255859]\n",
      "\t a: [1, 1999, 0.6183331298828125]\n",
      "\t l: [1, 2199, 2.2448553466796874]\n",
      "\t a: [1, 2199, 0.6272917175292969]\n",
      "\t l: [1, 2399, 2.1811985778808594]\n",
      "\t a: [1, 2399, 0.6629166412353515]\n",
      "\t Validation\n",
      "\t l: [1, 416, 4.487494812011719]\n",
      "\t a: [1, 416, 1.3826040649414062]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.1478973388671876]\n",
      "\t a: [2, 199, 0.6583332824707031]\n",
      "\t l: [2, 399, 2.1297567749023436]\n",
      "\t a: [2, 399, 0.6658335876464844]\n",
      "\t l: [2, 599, 2.1067886352539062]\n",
      "\t a: [2, 599, 0.6689584350585938]\n",
      "\t l: [2, 799, 2.1233636474609376]\n",
      "\t a: [2, 799, 0.6547916412353516]\n",
      "\t l: [2, 999, 2.0374095153808596]\n",
      "\t a: [2, 999, 0.6897917175292969]\n",
      "\t l: [2, 1199, 2.030165100097656]\n",
      "\t a: [2, 1199, 0.6812501525878907]\n",
      "\t l: [2, 1399, 2.088737030029297]\n",
      "\t a: [2, 1399, 0.6629165649414063]\n",
      "\t l: [2, 1599, 1.993241424560547]\n",
      "\t a: [2, 1599, 0.6989585113525391]\n",
      "\t l: [2, 1799, 1.9824752807617188]\n",
      "\t a: [2, 1799, 0.6985417938232422]\n",
      "\t l: [2, 1999, 1.9722140502929688]\n",
      "\t a: [2, 1999, 0.7056251525878906]\n",
      "\t l: [2, 2199, 1.9456280517578124]\n",
      "\t a: [2, 2199, 0.7025001525878907]\n",
      "\t l: [2, 2399, 1.8851849365234374]\n",
      "\t a: [2, 2399, 0.7356250762939454]\n",
      "\t Validation\n",
      "\t l: [2, 416, 3.9013360595703124]\n",
      "\t a: [2, 416, 1.5268748474121094]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 1.8750715637207032]\n",
      "\t a: [3, 199, 0.7283330535888672]\n",
      "\t l: [3, 399, 1.8720780944824218]\n",
      "\t a: [3, 399, 0.7295832824707031]\n",
      "\t l: [3, 599, 1.856923828125]\n",
      "\t a: [3, 599, 0.7252080535888672]\n",
      "\t l: [3, 799, 1.8865037536621094]\n",
      "\t a: [3, 799, 0.7147917175292968]\n",
      "\t l: [3, 999, 1.8055526733398437]\n",
      "\t a: [3, 999, 0.7454166412353516]\n",
      "\t l: [3, 1199, 1.8077220153808593]\n",
      "\t a: [3, 1199, 0.7368749237060547]\n",
      "\t l: [3, 1399, 1.8815536499023438]\n",
      "\t a: [3, 1399, 0.7181252288818359]\n",
      "\t l: [3, 1599, 1.783433380126953]\n",
      "\t a: [3, 1599, 0.7504167175292968]\n",
      "\t l: [3, 1799, 1.7897157287597656]\n",
      "\t a: [3, 1799, 0.7495831298828125]\n",
      "\t l: [3, 1999, 1.778507080078125]\n",
      "\t a: [3, 1999, 0.7510416412353516]\n",
      "\t l: [3, 2199, 1.7598716735839843]\n",
      "\t a: [3, 2199, 0.745625]\n",
      "\t l: [3, 2399, 1.7017491149902344]\n",
      "\t a: [3, 2399, 0.7739582824707031]\n",
      "\t Validation\n",
      "\t l: [3, 416, 3.5354986572265625]\n",
      "\t a: [3, 416, 1.6087496948242188]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 1.7029914855957031]\n",
      "\t a: [4, 199, 0.7612496948242188]\n",
      "\t l: [4, 399, 1.7088624572753905]\n",
      "\t a: [4, 399, 0.7674998474121094]\n",
      "\t l: [4, 599, 1.697410125732422]\n",
      "\t a: [4, 599, 0.758333511352539]\n",
      "\t l: [4, 799, 1.7315611267089843]\n",
      "\t a: [4, 799, 0.7527084350585938]\n",
      "\t l: [4, 999, 1.6518545532226563]\n",
      "\t a: [4, 999, 0.7758330535888672]\n",
      "\t l: [4, 1199, 1.659248809814453]\n",
      "\t a: [4, 1199, 0.7697914123535157]\n",
      "\t l: [4, 1399, 1.7404638671875]\n",
      "\t a: [4, 1399, 0.7493748474121094]\n",
      "\t l: [4, 1599, 1.6400827026367188]\n",
      "\t a: [4, 1599, 0.7831249237060547]\n",
      "\t l: [4, 1799, 1.656638641357422]\n",
      "\t a: [4, 1799, 0.7787499237060547]\n",
      "\t l: [4, 1999, 1.6449636840820312]\n",
      "\t a: [4, 1999, 0.7749998474121094]\n",
      "\t l: [4, 2199, 1.6309214782714845]\n",
      "\t a: [4, 2199, 0.7691665649414062]\n",
      "\t l: [4, 2399, 1.5721438598632813]\n",
      "\t a: [4, 2399, 0.8010413360595703]\n",
      "\t Validation\n",
      "\t l: [4, 416, 3.2792864990234376]\n",
      "\t a: [4, 416, 1.6597914123535156]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 1.5806173706054687]\n",
      "\t a: [5, 199, 0.7864585113525391]\n",
      "\t l: [5, 399, 1.5929547119140626]\n",
      "\t a: [5, 399, 0.7939582824707031]\n",
      "\t l: [5, 599, 1.5850640869140624]\n",
      "\t a: [5, 599, 0.7841667175292969]\n",
      "\t l: [5, 799, 1.6209115600585937]\n",
      "\t a: [5, 799, 0.7760416412353516]\n",
      "\t l: [5, 999, 1.5450692749023438]\n",
      "\t a: [5, 999, 0.7937499237060547]\n",
      "\t l: [5, 1199, 1.5521243286132813]\n",
      "\t a: [5, 1199, 0.7947916412353515]\n",
      "\t l: [5, 1399, 1.640006103515625]\n",
      "\t a: [5, 1399, 0.7704165649414062]\n",
      "\t l: [5, 1599, 1.5368626403808594]\n",
      "\t a: [5, 1599, 0.799375228881836]\n",
      "\t l: [5, 1799, 1.5593167114257813]\n",
      "\t a: [5, 1799, 0.7927084350585938]\n",
      "\t l: [5, 1999, 1.5452110290527343]\n",
      "\t a: [5, 1999, 0.7918744659423829]\n",
      "\t l: [5, 2199, 1.533330535888672]\n",
      "\t a: [5, 2199, 0.7899996948242187]\n",
      "\t l: [5, 2399, 1.4760197448730468]\n",
      "\t a: [5, 2399, 0.8202082061767578]\n",
      "\t Validation\n",
      "\t l: [5, 416, 3.08425537109375]\n",
      "\t a: [5, 416, 1.6948960876464845]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 1.487149658203125]\n",
      "\t a: [6, 199, 0.8004167175292969]\n",
      "\t l: [6, 399, 1.502456512451172]\n",
      "\t a: [6, 399, 0.8093749237060547]\n",
      "\t l: [6, 599, 1.4963243103027344]\n",
      "\t a: [6, 599, 0.7968748474121093]\n",
      "\t l: [6, 799, 1.5333154296875]\n",
      "\t a: [6, 799, 0.7920833587646484]\n",
      "\t l: [6, 999, 1.4589718627929686]\n",
      "\t a: [6, 999, 0.8093750762939453]\n",
      "\t l: [6, 1199, 1.4680787658691405]\n",
      "\t a: [6, 1199, 0.8081248474121093]\n",
      "\t l: [6, 1399, 1.5595481872558594]\n",
      "\t a: [6, 1399, 0.7833332824707031]\n",
      "\t l: [6, 1599, 1.45503173828125]\n",
      "\t a: [6, 1599, 0.8139583587646484]\n",
      "\t l: [6, 1799, 1.4823881530761718]\n",
      "\t a: [6, 1799, 0.80625]\n",
      "\t l: [6, 1999, 1.4676934814453124]\n",
      "\t a: [6, 1999, 0.8049999237060547]\n",
      "\t l: [6, 2199, 1.4586941528320312]\n",
      "\t a: [6, 2199, 0.7997915649414062]\n",
      "\t l: [6, 2399, 1.40110107421875]\n",
      "\t a: [6, 2399, 0.8304167175292969]\n",
      "\t Validation\n",
      "\t l: [6, 416, 2.9301849365234376]\n",
      "\t a: [6, 416, 1.7222915649414063]\n",
      "--- Epoch : 7 ---\n",
      "\t Training\n",
      "\t l: [7, 199, 1.413669891357422]\n",
      "\t a: [7, 199, 0.8137500762939454]\n",
      "\t l: [7, 399, 1.4315089416503906]\n",
      "\t a: [7, 399, 0.819999771118164]\n",
      "\t l: [7, 599, 1.427353973388672]\n",
      "\t a: [7, 599, 0.8095832061767578]\n",
      "\t l: [7, 799, 1.4636289978027344]\n",
      "\t a: [7, 799, 0.8049999237060547]\n",
      "\t l: [7, 999, 1.3913583374023437]\n",
      "\t a: [7, 999, 0.8195832061767578]\n",
      "\t l: [7, 1199, 1.4008668518066407]\n",
      "\t a: [7, 1199, 0.8185414123535156]\n",
      "\t l: [7, 1399, 1.4939805603027343]\n",
      "\t a: [7, 1399, 0.7941667175292969]\n",
      "\t l: [7, 1599, 1.3897335815429688]\n",
      "\t a: [7, 1599, 0.8237499237060547]\n",
      "\t l: [7, 1799, 1.4208052062988281]\n",
      "\t a: [7, 1799, 0.8185415649414063]\n",
      "\t l: [7, 1999, 1.4056788635253907]\n",
      "\t a: [7, 1999, 0.8158333587646485]\n",
      "\t l: [7, 2199, 1.397557830810547]\n",
      "\t a: [7, 2199, 0.8112499237060546]\n",
      "\t l: [7, 2399, 1.340015869140625]\n",
      "\t a: [7, 2399, 0.8381251525878907]\n",
      "\t Validation\n",
      "\t l: [7, 416, 2.806615905761719]\n",
      "\t a: [7, 416, 1.7377081298828125]\n",
      "--- Epoch : 8 ---\n",
      "\t Training\n",
      "\t l: [8, 199, 1.3547146606445313]\n",
      "\t a: [8, 199, 0.8231250762939453]\n",
      "\t l: [8, 399, 1.373099822998047]\n",
      "\t a: [8, 399, 0.8281247711181641]\n",
      "\t l: [8, 599, 1.3726300048828124]\n",
      "\t a: [8, 599, 0.8210415649414062]\n",
      "\t l: [8, 799, 1.4080502319335937]\n",
      "\t a: [8, 799, 0.8131250762939453]\n",
      "\t l: [8, 999, 1.3375660705566406]\n",
      "\t a: [8, 999, 0.82625]\n",
      "\t l: [8, 1199, 1.3477682495117187]\n",
      "\t a: [8, 1199, 0.8252079010009765]\n",
      "\t l: [8, 1399, 1.44265869140625]\n",
      "\t a: [8, 1399, 0.8012501525878907]\n",
      "\t l: [8, 1599, 1.3366053771972657]\n",
      "\t a: [8, 1599, 0.8295832824707031]\n",
      "\t l: [8, 1799, 1.3699525451660157]\n",
      "\t a: [8, 1799, 0.8243749237060547]\n",
      "\t l: [8, 1999, 1.3544876098632812]\n",
      "\t a: [8, 1999, 0.8218748474121094]\n",
      "\t l: [8, 2199, 1.3478958129882812]\n",
      "\t a: [8, 2199, 0.8191667175292969]\n",
      "\t l: [8, 2399, 1.2902320861816405]\n",
      "\t a: [8, 2399, 0.8443751525878906]\n",
      "\t Validation\n",
      "\t l: [8, 416, 2.706548156738281]\n",
      "\t a: [8, 416, 1.7483331298828124]\n",
      "--- Epoch : 9 ---\n",
      "\t Training\n",
      "\t l: [9, 199, 1.307110595703125]\n",
      "\t a: [9, 199, 0.8293753814697266]\n",
      "\t l: [9, 399, 1.326532745361328]\n",
      "\t a: [9, 399, 0.8341663360595704]\n",
      "\t l: [9, 599, 1.3276417541503907]\n",
      "\t a: [9, 599, 0.8287498474121093]\n",
      "\t l: [9, 799, 1.3606192016601562]\n",
      "\t a: [9, 799, 0.820416488647461]\n",
      "\t l: [9, 999, 1.2910438537597657]\n",
      "\t a: [9, 999, 0.8320832824707032]\n",
      "\t l: [9, 1199, 1.302383575439453]\n",
      "\t a: [9, 1199, 0.8308331298828125]\n",
      "\t l: [9, 1399, 1.3980003356933595]\n",
      "\t a: [9, 1399, 0.8072917938232422]\n",
      "\t l: [9, 1599, 1.2918914794921874]\n",
      "\t a: [9, 1599, 0.8366667175292969]\n",
      "\t l: [9, 1799, 1.327501983642578]\n",
      "\t a: [9, 1799, 0.8295832824707031]\n",
      "\t l: [9, 1999, 1.3112103271484374]\n",
      "\t a: [9, 1999, 0.8274998474121094]\n",
      "\t l: [9, 2199, 1.305128173828125]\n",
      "\t a: [9, 2199, 0.8258332824707031]\n",
      "\t l: [9, 2399, 1.2470748138427734]\n",
      "\t a: [9, 2399, 0.8483335113525391]\n",
      "\t Validation\n",
      "\t l: [9, 416, 2.614682922363281]\n",
      "\t a: [9, 416, 1.7587501525878906]\n",
      "--- Epoch : 10 ---\n",
      "\t Training\n",
      "\t l: [10, 199, 1.2632711791992188]\n",
      "\t a: [10, 199, 0.836875228881836]\n",
      "\t l: [10, 399, 1.2849038696289063]\n",
      "\t a: [10, 399, 0.8374998474121094]\n",
      "\t l: [10, 599, 1.2870309448242188]\n",
      "\t a: [10, 599, 0.8337498474121093]\n",
      "\t l: [10, 799, 1.320358123779297]\n",
      "\t a: [10, 799, 0.8256244659423828]\n",
      "\t l: [10, 999, 1.2521489715576173]\n",
      "\t a: [10, 999, 0.8372915649414062]\n",
      "\t l: [10, 1199, 1.2638266754150391]\n",
      "\t a: [10, 1199, 0.836875]\n",
      "\t l: [10, 1399, 1.3602645874023438]\n",
      "\t a: [10, 1399, 0.8118749237060547]\n",
      "\t l: [10, 1599, 1.2527191162109375]\n",
      "\t a: [10, 1599, 0.8433332824707032]\n",
      "\t l: [10, 1799, 1.2907290649414063]\n",
      "\t a: [10, 1799, 0.8339582824707031]\n",
      "\t l: [10, 1999, 1.2739295196533202]\n",
      "\t a: [10, 1999, 0.8362498474121094]\n",
      "\t l: [10, 2199, 1.2687864685058594]\n",
      "\t a: [10, 2199, 0.8304165649414063]\n",
      "\t l: [10, 2399, 1.211944580078125]\n",
      "\t a: [10, 2399, 0.8516667175292969]\n",
      "\t Validation\n",
      "\t l: [10, 416, 2.5418251037597654]\n",
      "\t a: [10, 416, 1.7697918701171875]\n",
      "--- Epoch : 11 ---\n",
      "\t Training\n",
      "\t l: [11, 199, 1.2273970031738282]\n",
      "\t a: [11, 199, 0.8429170227050782]\n",
      "\t l: [11, 399, 1.2495536041259765]\n",
      "\t a: [11, 399, 0.8437501525878907]\n",
      "\t l: [11, 599, 1.2527732849121094]\n",
      "\t a: [11, 599, 0.8374999237060546]\n",
      "\t l: [11, 799, 1.2846511840820312]\n",
      "\t a: [11, 799, 0.8304165649414063]\n",
      "\t l: [11, 999, 1.2184247589111328]\n",
      "\t a: [11, 999, 0.8402083587646484]\n",
      "\t l: [11, 1199, 1.2305234527587892]\n",
      "\t a: [11, 1199, 0.8406250762939453]\n",
      "\t l: [11, 1399, 1.3272091674804687]\n",
      "\t a: [11, 1399, 0.815]\n",
      "\t l: [11, 1599, 1.2195880889892579]\n",
      "\t a: [11, 1599, 0.846875]\n",
      "\t l: [11, 1799, 1.2594144439697266]\n",
      "\t a: [11, 1799, 0.8385417175292968]\n",
      "\t l: [11, 1999, 1.241220932006836]\n",
      "\t a: [11, 1999, 0.8391664886474609]\n",
      "\t l: [11, 2199, 1.2374961853027344]\n",
      "\t a: [11, 2199, 0.834374771118164]\n",
      "\t l: [11, 2399, 1.1800670623779297]\n",
      "\t a: [11, 2399, 0.8543748474121093]\n",
      "\t Validation\n",
      "\t l: [11, 416, 2.4769410705566406]\n",
      "\t a: [11, 416, 1.7772915649414063]\n",
      "--- Epoch : 12 ---\n",
      "\t Training\n",
      "\t l: [12, 199, 1.1955352783203126]\n",
      "\t a: [12, 199, 0.8485419464111328]\n",
      "\t l: [12, 399, 1.2186448669433594]\n",
      "\t a: [12, 399, 0.8479168701171875]\n",
      "\t l: [12, 599, 1.2236177825927734]\n",
      "\t a: [12, 599, 0.8391664123535156]\n",
      "\t l: [12, 799, 1.2545632171630858]\n",
      "\t a: [12, 799, 0.8362496948242187]\n",
      "\t l: [12, 999, 1.1894049072265624]\n",
      "\t a: [12, 999, 0.8431248474121094]\n",
      "\t l: [12, 1199, 1.2028472900390625]\n",
      "\t a: [12, 1199, 0.8431251525878907]\n",
      "\t l: [12, 1399, 1.298365478515625]\n",
      "\t a: [12, 1399, 0.8191667938232422]\n",
      "\t l: [12, 1599, 1.1914835357666016]\n",
      "\t a: [12, 1599, 0.8481248474121094]\n",
      "\t l: [12, 1799, 1.2321378326416015]\n",
      "\t a: [12, 1799, 0.8410416412353515]\n",
      "\t l: [12, 1999, 1.214310073852539]\n",
      "\t a: [12, 1999, 0.8433332061767578]\n",
      "\t l: [12, 2199, 1.2103752899169922]\n",
      "\t a: [12, 2199, 0.8364581298828125]\n",
      "\t l: [12, 2399, 1.1535501861572266]\n",
      "\t a: [12, 2399, 0.8599998474121093]\n",
      "\t Validation\n",
      "\t l: [12, 416, 2.42171875]\n",
      "\t a: [12, 416, 1.7839582824707032]\n",
      "--- Epoch : 13 ---\n",
      "\t Training\n",
      "\t l: [13, 199, 1.1688629150390626]\n",
      "\t a: [13, 199, 0.8506252288818359]\n",
      "\t l: [13, 399, 1.1921849822998047]\n",
      "\t a: [13, 399, 0.850833511352539]\n",
      "\t l: [13, 599, 1.1983775329589843]\n",
      "\t a: [13, 599, 0.8406249237060547]\n",
      "\t l: [13, 799, 1.2294532012939454]\n",
      "\t a: [13, 799, 0.8395830535888672]\n",
      "\t l: [13, 999, 1.1648798370361328]\n",
      "\t a: [13, 999, 0.846249771118164]\n",
      "\t l: [13, 1199, 1.1781629943847656]\n",
      "\t a: [13, 1199, 0.8475001525878906]\n",
      "\t l: [13, 1399, 1.2741963958740234]\n",
      "\t a: [13, 1399, 0.8220833587646484]\n",
      "\t l: [13, 1599, 1.165842056274414]\n",
      "\t a: [13, 1599, 0.8514582824707031]\n",
      "\t l: [13, 1799, 1.2073883819580078]\n",
      "\t a: [13, 1799, 0.8420832824707031]\n",
      "\t l: [13, 1999, 1.1908264923095704]\n",
      "\t a: [13, 1999, 0.8450000762939454]\n",
      "\t l: [13, 2199, 1.1866753387451172]\n",
      "\t a: [13, 2199, 0.8408332824707031]\n",
      "\t l: [13, 2399, 1.1303083801269531]\n",
      "\t a: [13, 2399, 0.8604166412353516]\n",
      "\t Validation\n",
      "\t l: [13, 416, 2.373115234375]\n",
      "\t a: [13, 416, 1.7935414123535156]\n",
      "--- Epoch : 14 ---\n",
      "\t Training\n",
      "\t l: [14, 199, 1.1441764068603515]\n",
      "\t a: [14, 199, 0.8535419464111328]\n",
      "\t l: [14, 399, 1.1692384338378907]\n",
      "\t a: [14, 399, 0.8545834350585938]\n",
      "\t l: [14, 599, 1.1748929595947266]\n",
      "\t a: [14, 599, 0.8443750762939453]\n",
      "\t l: [14, 799, 1.2052918243408204]\n",
      "\t a: [14, 799, 0.8399995422363281]\n",
      "\t l: [14, 999, 1.1423214721679686]\n",
      "\t a: [14, 999, 0.8483330535888672]\n",
      "\t l: [14, 1199, 1.1560677337646483]\n",
      "\t a: [14, 1199, 0.8485416412353516]\n",
      "\t l: [14, 1399, 1.2522205352783202]\n",
      "\t a: [14, 1399, 0.8239583587646484]\n",
      "\t l: [14, 1599, 1.1439216613769532]\n",
      "\t a: [14, 1599, 0.8533332824707032]\n",
      "\t l: [14, 1799, 1.186141815185547]\n",
      "\t a: [14, 1799, 0.8454167175292969]\n",
      "\t l: [14, 1999, 1.1686852264404297]\n",
      "\t a: [14, 1999, 0.8474998474121094]\n",
      "\t l: [14, 2199, 1.1648363494873046]\n",
      "\t a: [14, 2199, 0.8427083587646484]\n",
      "\t l: [14, 2399, 1.1081503295898438]\n",
      "\t a: [14, 2399, 0.8629166412353516]\n",
      "\t Validation\n",
      "\t l: [14, 416, 2.3279258728027346]\n",
      "\t a: [14, 416, 1.797083282470703]\n",
      "--- Epoch : 15 ---\n",
      "\t Training\n",
      "\t l: [15, 199, 1.1217394256591797]\n",
      "\t a: [15, 199, 0.8575001525878906]\n",
      "\t l: [15, 399, 1.147303924560547]\n",
      "\t a: [15, 399, 0.8568749237060547]\n",
      "\t l: [15, 599, 1.1537183380126954]\n",
      "\t a: [15, 599, 0.8477085113525391]\n",
      "\t l: [15, 799, 1.1833737182617188]\n",
      "\t a: [15, 799, 0.8441663360595704]\n",
      "\t l: [15, 999, 1.1214256286621094]\n",
      "\t a: [15, 999, 0.8539579772949218]\n",
      "\t l: [15, 1199, 1.1340691375732421]\n",
      "\t a: [15, 1199, 0.851458511352539]\n",
      "\t l: [15, 1399, 1.2313468170166015]\n",
      "\t a: [15, 1399, 0.8264582824707031]\n",
      "\t l: [15, 1599, 1.1220429992675782]\n",
      "\t a: [15, 1599, 0.8574999237060547]\n",
      "\t l: [15, 1799, 1.1657878875732421]\n",
      "\t a: [15, 1799, 0.8483334350585937]\n",
      "\t l: [15, 1999, 1.1477559661865235]\n",
      "\t a: [15, 1999, 0.8504167938232422]\n",
      "\t l: [15, 2199, 1.1441405487060547]\n",
      "\t a: [15, 2199, 0.8454167175292969]\n",
      "\t l: [15, 2399, 1.0870509338378906]\n",
      "\t a: [15, 2399, 0.8652083587646484]\n",
      "\t Validation\n",
      "\t l: [15, 416, 2.2845307922363283]\n",
      "\t a: [15, 416, 1.8018748474121093]\n",
      "--- Epoch : 16 ---\n",
      "\t Training\n",
      "\t l: [16, 199, 1.1006864166259767]\n",
      "\t a: [16, 199, 0.8600000762939453]\n",
      "\t l: [16, 399, 1.1271863555908204]\n",
      "\t a: [16, 399, 0.8595832061767578]\n",
      "\t l: [16, 599, 1.134576873779297]\n",
      "\t a: [16, 599, 0.8495834350585938]\n",
      "\t l: [16, 799, 1.1635511779785157]\n",
      "\t a: [16, 799, 0.8483329772949219]\n",
      "\t l: [16, 999, 1.101873321533203]\n",
      "\t a: [16, 999, 0.855624771118164]\n",
      "\t l: [16, 1199, 1.1150179290771485]\n",
      "\t a: [16, 1199, 0.8537501525878907]\n",
      "\t l: [16, 1399, 1.2126921844482421]\n",
      "\t a: [16, 1399, 0.8283333587646484]\n",
      "\t l: [16, 1599, 1.1043311309814454]\n",
      "\t a: [16, 1599, 0.8585417938232421]\n",
      "\t l: [16, 1799, 1.1479226684570312]\n",
      "\t a: [16, 1799, 0.8497916412353516]\n",
      "\t l: [16, 1999, 1.1291614532470704]\n",
      "\t a: [16, 1999, 0.8514583587646485]\n",
      "\t l: [16, 2199, 1.1261641693115234]\n",
      "\t a: [16, 2199, 0.8466667175292969]\n",
      "\t l: [16, 2399, 1.0684727478027343]\n",
      "\t a: [16, 2399, 0.8664582824707031]\n",
      "\t Validation\n",
      "\t l: [16, 416, 2.2471578979492186]\n",
      "\t a: [16, 416, 1.8072920227050782]\n",
      "--- Epoch : 17 ---\n",
      "\t Training\n",
      "\t l: [17, 199, 1.082595977783203]\n",
      "\t a: [17, 199, 0.8610417175292969]\n",
      "\t l: [17, 399, 1.1097925567626954]\n",
      "\t a: [17, 399, 0.8614583587646485]\n",
      "\t l: [17, 599, 1.1186478424072266]\n",
      "\t a: [17, 599, 0.851875]\n",
      "\t l: [17, 799, 1.1462261962890625]\n",
      "\t a: [17, 799, 0.8502079772949219]\n",
      "\t l: [17, 999, 1.0850861358642578]\n",
      "\t a: [17, 999, 0.856041488647461]\n",
      "\t l: [17, 1199, 1.0988772583007813]\n",
      "\t a: [17, 1199, 0.8556251525878906]\n",
      "\t l: [17, 1399, 1.1966473388671874]\n",
      "\t a: [17, 1399, 0.8304167175292969]\n",
      "\t l: [17, 1599, 1.0882658386230468]\n",
      "\t a: [17, 1599, 0.8618751525878906]\n",
      "\t l: [17, 1799, 1.1316840362548828]\n",
      "\t a: [17, 1799, 0.8512500762939453]\n",
      "\t l: [17, 1999, 1.1132617950439454]\n",
      "\t a: [17, 1999, 0.8527083587646485]\n",
      "\t l: [17, 2199, 1.110499496459961]\n",
      "\t a: [17, 2199, 0.8487500762939453]\n",
      "\t l: [17, 2399, 1.0532802581787108]\n",
      "\t a: [17, 2399, 0.8670832824707031]\n",
      "\t Validation\n",
      "\t l: [17, 416, 2.2144964599609374]\n",
      "\t a: [17, 416, 1.808958740234375]\n",
      "--- Epoch : 0 ---\n",
      "\t Training\n",
      "\t l: [0, 199, 4.4853515625]\n",
      "\t a: [0, 199, 0.09479165077209473]\n",
      "\t l: [0, 399, 3.6045867919921877]\n",
      "\t a: [0, 399, 0.08604167938232422]\n",
      "\t l: [0, 599, 3.4641104125976563]\n",
      "\t a: [0, 599, 0.11916665077209472]\n",
      "\t l: [0, 799, 3.3815957641601564]\n",
      "\t a: [0, 799, 0.12291671752929688]\n",
      "\t l: [0, 999, 3.31615478515625]\n",
      "\t a: [0, 999, 0.14291666984558105]\n",
      "\t l: [0, 1199, 3.213368225097656]\n",
      "\t a: [0, 1199, 0.17875003814697266]\n",
      "\t l: [0, 1399, 3.1394058227539063]\n",
      "\t a: [0, 1399, 0.21604162216186523]\n",
      "\t l: [0, 1599, 3.0694091796875]\n",
      "\t a: [0, 1599, 0.2504166221618652]\n",
      "\t l: [0, 1799, 2.9889492797851562]\n",
      "\t a: [0, 1799, 0.28958330154418943]\n",
      "\t l: [0, 1999, 2.916295166015625]\n",
      "\t a: [0, 1999, 0.32916664123535155]\n",
      "\t l: [0, 2199, 2.877809753417969]\n",
      "\t a: [0, 2199, 0.3393749237060547]\n",
      "\t l: [0, 2399, 2.801564025878906]\n",
      "\t a: [0, 2399, 0.37583332061767577]\n",
      "\t Validation\n",
      "\t l: [0, 416, 5.716346435546875]\n",
      "\t a: [0, 416, 0.8505204010009766]\n",
      "--- Epoch : 1 ---\n",
      "\t Training\n",
      "\t l: [1, 199, 2.7060040283203124]\n",
      "\t a: [1, 199, 0.42312496185302734]\n",
      "\t l: [1, 399, 2.6672396850585938]\n",
      "\t a: [1, 399, 0.4410417175292969]\n",
      "\t l: [1, 599, 2.610889587402344]\n",
      "\t a: [1, 599, 0.47083339691162107]\n",
      "\t l: [1, 799, 2.6150521850585937]\n",
      "\t a: [1, 799, 0.46708328247070313]\n",
      "\t l: [1, 999, 2.5332905578613283]\n",
      "\t a: [1, 999, 0.5002085113525391]\n",
      "\t l: [1, 1199, 2.4799180603027344]\n",
      "\t a: [1, 1199, 0.5358333206176757]\n",
      "\t l: [1, 1399, 2.4945303344726564]\n",
      "\t a: [1, 1399, 0.5229166412353515]\n",
      "\t l: [1, 1599, 2.412152862548828]\n",
      "\t a: [1, 1599, 0.5595833206176758]\n",
      "\t l: [1, 1799, 2.376819610595703]\n",
      "\t a: [1, 1799, 0.5762499237060547]\n",
      "\t l: [1, 1999, 2.3259236145019533]\n",
      "\t a: [1, 1999, 0.6081250381469726]\n",
      "\t l: [1, 2199, 2.309681091308594]\n",
      "\t a: [1, 2199, 0.6043749237060547]\n",
      "\t l: [1, 2399, 2.251522216796875]\n",
      "\t a: [1, 2399, 0.6416667175292968]\n",
      "\t Validation\n",
      "\t l: [1, 416, 4.637565002441407]\n",
      "\t a: [1, 416, 1.322395782470703]\n",
      "--- Epoch : 2 ---\n",
      "\t Training\n",
      "\t l: [2, 199, 2.204543151855469]\n",
      "\t a: [2, 199, 0.6404167175292969]\n",
      "\t l: [2, 399, 2.191573028564453]\n",
      "\t a: [2, 399, 0.6385416412353515]\n",
      "\t l: [2, 599, 2.157037658691406]\n",
      "\t a: [2, 599, 0.6560417938232422]\n",
      "\t l: [2, 799, 2.190386962890625]\n",
      "\t a: [2, 799, 0.6506249237060547]\n",
      "\t l: [2, 999, 2.113186492919922]\n",
      "\t a: [2, 999, 0.6637499237060547]\n",
      "\t l: [2, 1199, 2.0832365417480467]\n",
      "\t a: [2, 1199, 0.6804167175292969]\n",
      "\t l: [2, 1399, 2.1375804138183594]\n",
      "\t a: [2, 1399, 0.6516667175292968]\n",
      "\t l: [2, 1599, 2.0473114013671876]\n",
      "\t a: [2, 1599, 0.6933335876464843]\n",
      "\t l: [2, 1799, 2.0398048400878905]\n",
      "\t a: [2, 1799, 0.6906247711181641]\n",
      "\t l: [2, 1999, 1.9949185180664062]\n",
      "\t a: [2, 1999, 0.7124998474121094]\n",
      "\t l: [2, 2199, 1.9862051391601563]\n",
      "\t a: [2, 2199, 0.7114584350585937]\n",
      "\t l: [2, 2399, 1.9360276794433593]\n",
      "\t a: [2, 2399, 0.7368749237060547]\n",
      "\t Validation\n",
      "\t l: [2, 416, 4.0108795166015625]\n",
      "\t a: [2, 416, 1.518542022705078]\n",
      "--- Epoch : 3 ---\n",
      "\t Training\n",
      "\t l: [3, 199, 1.9115977478027344]\n",
      "\t a: [3, 199, 0.7304167938232422]\n",
      "\t l: [3, 399, 1.9125558471679687]\n",
      "\t a: [3, 399, 0.7289584350585937]\n",
      "\t l: [3, 599, 1.8887626647949218]\n",
      "\t a: [3, 599, 0.729375]\n",
      "\t l: [3, 799, 1.93443603515625]\n",
      "\t a: [3, 799, 0.7258332824707031]\n",
      "\t l: [3, 999, 1.8609422302246095]\n",
      "\t a: [3, 999, 0.731875]\n",
      "\t l: [3, 1199, 1.8411819458007812]\n",
      "\t a: [3, 1199, 0.7437498474121094]\n",
      "\t l: [3, 1399, 1.9165614318847657]\n",
      "\t a: [3, 1399, 0.7129170227050782]\n",
      "\t l: [3, 1599, 1.819744873046875]\n",
      "\t a: [3, 1599, 0.7527084350585938]\n",
      "\t l: [3, 1799, 1.8263340759277344]\n",
      "\t a: [3, 1799, 0.7416668701171875]\n",
      "\t l: [3, 1999, 1.7865007019042969]\n",
      "\t a: [3, 1999, 0.7587498474121094]\n",
      "\t l: [3, 2199, 1.7835671997070313]\n",
      "\t a: [3, 2199, 0.7531250762939453]\n",
      "\t l: [3, 2399, 1.7363662719726562]\n",
      "\t a: [3, 2399, 0.7781250762939453]\n",
      "\t Validation\n",
      "\t l: [3, 416, 3.612341003417969]\n",
      "\t a: [3, 416, 1.6080206298828126]\n",
      "--- Epoch : 4 ---\n",
      "\t Training\n",
      "\t l: [4, 199, 1.725440673828125]\n",
      "\t a: [4, 199, 0.7662501525878906]\n",
      "\t l: [4, 399, 1.732805938720703]\n",
      "\t a: [4, 399, 0.7656249237060547]\n",
      "\t l: [4, 599, 1.7159295654296876]\n",
      "\t a: [4, 599, 0.7677082824707031]\n",
      "\t l: [4, 799, 1.7646464538574218]\n",
      "\t a: [4, 799, 0.7554167175292968]\n",
      "\t l: [4, 999, 1.6936712646484375]\n",
      "\t a: [4, 999, 0.7683331298828125]\n",
      "\t l: [4, 1199, 1.679852752685547]\n",
      "\t a: [4, 1199, 0.7772915649414063]\n",
      "\t l: [4, 1399, 1.7656146240234376]\n",
      "\t a: [4, 1399, 0.7437499237060546]\n",
      "\t l: [4, 1599, 1.6665972900390624]\n",
      "\t a: [4, 1599, 0.7810414886474609]\n",
      "\t l: [4, 1799, 1.684329833984375]\n",
      "\t a: [4, 1799, 0.7693750762939453]\n",
      "\t l: [4, 1999, 1.6462588500976563]\n",
      "\t a: [4, 1999, 0.7814583587646484]\n",
      "\t l: [4, 2199, 1.645899658203125]\n",
      "\t a: [4, 2199, 0.7731248474121094]\n",
      "\t l: [4, 2399, 1.5999125671386718]\n",
      "\t a: [4, 2399, 0.7987498474121094]\n",
      "\t Validation\n",
      "\t l: [4, 416, 3.335918273925781]\n",
      "\t a: [4, 416, 1.6556251525878907]\n",
      "--- Epoch : 5 ---\n",
      "\t Training\n",
      "\t l: [5, 199, 1.5962005615234376]\n",
      "\t a: [5, 199, 0.7872919464111328]\n",
      "\t l: [5, 399, 1.6082759094238281]\n",
      "\t a: [5, 399, 0.7895833587646485]\n",
      "\t l: [5, 599, 1.5963230895996094]\n",
      "\t a: [5, 599, 0.7868755340576172]\n",
      "\t l: [5, 799, 1.6447723388671875]\n",
      "\t a: [5, 799, 0.7752085113525391]\n",
      "\t l: [5, 999, 1.57669921875]\n",
      "\t a: [5, 999, 0.7870832824707031]\n",
      "\t l: [5, 1199, 1.5678010559082032]\n",
      "\t a: [5, 1199, 0.7939582824707031]\n",
      "\t l: [5, 1399, 1.6601387023925782]\n",
      "\t a: [5, 1399, 0.7633332824707031]\n",
      "\t l: [5, 1599, 1.556815643310547]\n",
      "\t a: [5, 1599, 0.8027083587646484]\n",
      "\t l: [5, 1799, 1.5812667846679687]\n",
      "\t a: [5, 1799, 0.7843749237060547]\n",
      "\t l: [5, 1999, 1.5440972900390626]\n",
      "\t a: [5, 1999, 0.7970832824707031]\n",
      "\t l: [5, 2199, 1.5466712951660155]\n",
      "\t a: [5, 2199, 0.7889585876464844]\n",
      "\t l: [5, 2399, 1.5008248901367187]\n",
      "\t a: [5, 2399, 0.8112497711181641]\n",
      "\t Validation\n",
      "\t l: [5, 416, 3.136524963378906]\n",
      "\t a: [5, 416, 1.68625]\n",
      "--- Epoch : 6 ---\n",
      "\t Training\n",
      "\t l: [6, 199, 1.5015467834472656]\n",
      "\t a: [6, 199, 0.8022917175292968]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=17'>18</a>\u001b[0m d_fwd_train_batch \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, d)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=19'>20</a>\u001b[0m m \u001b[39m=\u001b[39m LogisticRegressor()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=20'>21</a>\u001b[0m vals \u001b[39m=\u001b[39m train_loop(m,train_generator, test_generator, logisticLoss, num_epochs, d_fwd_train_batch, \u001b[39m1e-4\u001b[39;49m, \u001b[39m200\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../logs/r1_\u001b[39m\u001b[39m{\u001b[39;00md\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=22'>23</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(vals, f)\n",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 6'\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_generator, validation_generator, loss, nepochs, batchIteration, lr, log_every_n_batches)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=51'>52</a>\u001b[0m key \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=52'>53</a>\u001b[0m one_hot_y \u001b[39m=\u001b[39m one_hot(y,n_targets)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=53'>54</a>\u001b[0m params \u001b[39m=\u001b[39m batchIteration(params,x,one_hot_y,lossFn, lr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=54'>55</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(x, params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=55'>56</a>\u001b[0m l \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss(one_hot_y,y_hat)\n",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 11'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(params, x, y, lossFn, lr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=13'>14</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(vals, f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=17'>18</a>\u001b[0m     d_fwd_train_batch \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, d)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=19'>20</a>\u001b[0m     m \u001b[39m=\u001b[39m LogisticRegressor()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000010?line=20'>21</a>\u001b[0m     vals \u001b[39m=\u001b[39m train_loop(m,train_generator, test_generator, logisticLoss, num_epochs, d_fwd_train_batch, \u001b[39m1e-4\u001b[39m, \u001b[39m200\u001b[39m)\n",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 9'\u001b[0m in \u001b[0;36mmultiple_fwd_train_batch\u001b[0;34m(params, x, y, lossFn, lr, dirs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=3'>4</a>\u001b[0m key, _ \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=4'>5</a>\u001b[0m v \u001b[39m=\u001b[39m sample_random_direction(params, normalize\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=5'>6</a>\u001b[0m _, j \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mjvp(\u001b[39mlambda\u001b[39;49;00m p: lossFn(p, x, y), (params, ), (v,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=6'>7</a>\u001b[0m grad_now \u001b[39m=\u001b[39m [(j \u001b[39m*\u001b[39m dw, j \u001b[39m*\u001b[39m db) \u001b[39mfor\u001b[39;00m (dw, db) \u001b[39min\u001b[39;00m v]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/api.py:2212\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(fun, primals, tangents, has_aux)\u001b[0m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2173'>2174</a>\u001b[0m \u001b[39m\"\"\"Computes a (forward-mode) Jacobian-vector product of ``fun``.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2174'>2175</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2175'>2176</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2208'>2209</a>\u001b[0m \u001b[39m0.19900084\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2209'>2210</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2210'>2211</a>\u001b[0m _check_callable(fun)\n\u001b[0;32m-> <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2211'>2212</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _jvp(lu\u001b[39m.\u001b[39;49mwrap_init(fun), primals, tangents, has_aux\u001b[39m=\u001b[39;49mhas_aux)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/api.py:2241\u001b[0m, in \u001b[0;36m_jvp\u001b[0;34m(fun, primals, tangents, has_aux)\u001b[0m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2238'>2239</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_aux:\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2239'>2240</a>\u001b[0m   flat_fun, out_tree \u001b[39m=\u001b[39m flatten_fun_nokwargs(fun, tree_def)\n\u001b[0;32m-> <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2240'>2241</a>\u001b[0m   out_primals, out_tangents \u001b[39m=\u001b[39m ad\u001b[39m.\u001b[39;49mjvp(flat_fun)\u001b[39m.\u001b[39;49mcall_wrapped(ps_flat, ts_flat)\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2241'>2242</a>\u001b[0m   out_tree \u001b[39m=\u001b[39m out_tree()\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2242'>2243</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m (tree_unflatten(out_tree, out_primals),\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=2243'>2244</a>\u001b[0m           tree_unflatten(out_tree, out_tangents))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/linear_util.py:168\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=164'>165</a>\u001b[0m gen \u001b[39m=\u001b[39m gen_static_args \u001b[39m=\u001b[39m out_store \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=166'>167</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=167'>168</a>\u001b[0m   ans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=168'>169</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=169'>170</a>\u001b[0m   \u001b[39m# Some transformations yield from inside context managers, so we have to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=170'>171</a>\u001b[0m   \u001b[39m# interrupt them before reraising the exception. Otherwise they will only\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=171'>172</a>\u001b[0m   \u001b[39m# get garbage-collected at some later time, running their cleanup tasks\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=172'>173</a>\u001b[0m   \u001b[39m# only after this exception is handled, which can corrupt the global\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=173'>174</a>\u001b[0m   \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/linear_util.py?line=174'>175</a>\u001b[0m   \u001b[39mwhile\u001b[39;00m stack:\n",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 9'\u001b[0m in \u001b[0;36mmultiple_fwd_train_batch.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=3'>4</a>\u001b[0m key, _ \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(key)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=4'>5</a>\u001b[0m v \u001b[39m=\u001b[39m sample_random_direction(params, normalize\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=5'>6</a>\u001b[0m _, j \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mjvp(\u001b[39mlambda\u001b[39;00m p: lossFn(p, x, y), (params, ), (v,))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=6'>7</a>\u001b[0m grad_now \u001b[39m=\u001b[39m [(j \u001b[39m*\u001b[39m dw, j \u001b[39m*\u001b[39m db) \u001b[39mfor\u001b[39;00m (dw, db) \u001b[39min\u001b[39;00m v]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 6'\u001b[0m in \u001b[0;36mtrain_loop.<locals>.lossFn\u001b[0;34m(params, x, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlossFn\u001b[39m(params, x, y):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=42'>43</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m model(x,params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000005?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss(y,y_hat)\n",
      "File \u001b[0;32m~/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py:14\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, params)\n",
      "\u001b[1;32m/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb Cell 4'\u001b[0m in \u001b[0;36mLogisticRegressor.forward\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000003?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x,params):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Desktop/dssc/deeplearning/proj/DL-project/optim_strategies.ipynb#ch0000003?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x,params)\n",
      "File \u001b[0;32m~/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py:14\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, params)\n",
      "File \u001b[0;32m~/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py:411\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=408'>409</a>\u001b[0m out \u001b[39m=\u001b[39m x\n\u001b[1;32m    <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=409'>410</a>\u001b[0m \u001b[39mfor\u001b[39;00m p, c \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_components):\n\u001b[0;32m--> <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=410'>411</a>\u001b[0m     out \u001b[39m=\u001b[39m c(out, p)\n\u001b[1;32m    <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=412'>413</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py:14\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/module.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, params)\n",
      "File \u001b[0;32m~/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py:101\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=90'>91</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, params):\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=91'>92</a>\u001b[0m     \u001b[39m\"\"\"Public forward method for Linear layer\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=92'>93</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=93'>94</a>\u001b[0m \u001b[39m    :param params: Parameters of the layer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=98'>99</a>\u001b[0m \u001b[39m    :rtype: jnp.array\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=99'>100</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=100'>101</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(x, params)\n",
      "File \u001b[0;32m~/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py:89\u001b[0m, in \u001b[0;36mLinear._forward\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=85'>86</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mdot(x,params[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=86'>87</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/francesco/Desktop/dssc/deeplearning/proj/DL-project/veni/net.py?line=88'>89</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39;49mdot(x, params[\u001b[39m0\u001b[39;49m]) \u001b[39m+\u001b[39m params[\u001b[39m1\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/api.py:473\u001b[0m, in \u001b[0;36m_cpp_jit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=470'>471</a>\u001b[0m   in_type \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39minfer_lambda_input_type(\u001b[39mNone\u001b[39;00m, args_flat)\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=471'>472</a>\u001b[0m   flat_fun \u001b[39m=\u001b[39m lu\u001b[39m.\u001b[39mannotate(flat_fun, in_type)\n\u001b[0;32m--> <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=472'>473</a>\u001b[0m out_flat \u001b[39m=\u001b[39m xla\u001b[39m.\u001b[39;49mxla_call(\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=473'>474</a>\u001b[0m     flat_fun, \u001b[39m*\u001b[39;49margs_flat,\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=474'>475</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice, backend\u001b[39m=\u001b[39;49mbackend, name\u001b[39m=\u001b[39;49mflat_fun\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=475'>476</a>\u001b[0m     donated_invars\u001b[39m=\u001b[39;49mdonated_invars, inline\u001b[39m=\u001b[39;49minline, keep_unused\u001b[39m=\u001b[39;49mkeep_unused)\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=476'>477</a>\u001b[0m out_pytree_def \u001b[39m=\u001b[39m out_tree()\n\u001b[1;32m    <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/_src/api.py?line=477'>478</a>\u001b[0m out \u001b[39m=\u001b[39m tree_unflatten(out_pytree_def, out_flat)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/core.py:1767\u001b[0m, in \u001b[0;36mCallPrimitive.bind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/core.py?line=1765'>1766</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, fun, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m-> <a href='file:///home/francesco/.local/lib/python3.10/site-packages/jax/core.py?line=1766'>1767</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m call_bind(\u001b[39mself\u001b[39;49m, fun, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def params_norm(p):\n",
    "    norm = 0\n",
    "    for  w, b  in p:\n",
    "        norm += jnp.linalg.norm(b)**2 + jnp.linalg.norm(w)**2\n",
    "\n",
    "    return norm ** 0.5\n",
    "\n",
    "import pickle\n",
    "\n",
    "num_epochs = 18\n",
    "m = LogisticRegressor()\n",
    "vals = train_loop(m,train_generator, test_generator, logisticLoss, num_epochs, bwd_train_batch, 1e-4, 200)\n",
    "with open(f\"../logs/r1_b\",'wb') as f:\n",
    "    pickle.dump(vals, f)\n",
    "\n",
    "for d in [1, 10, 50, 100]:\n",
    "\n",
    "    print(f\"Training with {d} directions \\n \\n\")\n",
    "\n",
    "    d_fwd_train_batch = lambda params, x, y, lossFn, lr: multiple_fwd_train_batch(params, x, y, lossFn, lr, d)\n",
    "    \n",
    "    m = LogisticRegressor()\n",
    "    vals = train_loop(m,train_generator, test_generator, logisticLoss, num_epochs, d_fwd_train_batch, 1e-4, 200)\n",
    "    with open(f\"../logs/r1_{d}\",'wb') as f:\n",
    "        pickle.dump(vals, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
